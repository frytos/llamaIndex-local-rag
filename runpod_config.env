# ==========================================
# Runpod Configuration for RAG Pipeline
# ==========================================
# Source this file: source runpod_config.env
#
# Two LLM backends supported:
# 1. llama.cpp (GGUF models, CPU/GPU) - DEFAULT
# 2. vLLM (HuggingFace models, GPU-only) - FASTER! Set USE_VLLM=1
#
# For vLLM optimized config, use: source runpod_vllm_config.env

# ==========================================
# LLM Backend Selection
# ==========================================
export USE_VLLM=0                   # 0=llama.cpp, 1=vLLM (15x faster on GPU!)
# export VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ  # If USE_VLLM=1

# ==========================================
# GPU Configuration (NVIDIA CUDA)
# ==========================================
export EMBED_BACKEND=torch          # Use PyTorch instead of MLX
export CUDA_VISIBLE_DEVICES=0       # Use first GPU
export N_GPU_LAYERS=99              # Offload all LLM layers to GPU (llama.cpp only)

# ==========================================
# LLM Configuration (llama.cpp - Default)
# ==========================================
export N_BATCH=512                  # Large batch size (24GB VRAM)
export CTX=8192                     # Context window (increased for GPU)
export MAX_NEW_TOKENS=512           # Max tokens to generate
export TEMP=0.1                     # Temperature (0.0-1.0)
export N_THREADS=8                  # CPU threads (if needed)

# ==========================================
# Embedding Configuration (GPU-Optimized)
# ==========================================
export EMBED_MODEL=BAAI/bge-small-en
export EMBED_DIM=384
export EMBED_BATCH=256              # 2x larger than M1 (more VRAM)

# ==========================================
# PostgreSQL Configuration
# ==========================================
# Option 1: Local PostgreSQL on Runpod
export PGHOST=localhost
export PGPORT=5432

# Option 2: External managed PostgreSQL (recommended)
# export PGHOST=your-postgres-host.com
# export PGPORT=5432

export PGUSER=fryt
export PGPASSWORD=frytos
export DB_NAME=vector_db

# ==========================================
# RAG Configuration
# ==========================================
export CHUNK_SIZE=700
export CHUNK_OVERLAP=150
export TOP_K=5
export HYBRID_ALPHA=0.5
export MMR_THRESHOLD=0.5
export ENABLE_FILTERS=1

# ==========================================
# Logging & Debug (Enhanced)
# ==========================================
export LOG_LEVEL=INFO                       # INFO (default) or DEBUG (very verbose)
export LOG_FULL_CHUNKS=1                    # Show full retrieved chunks in logs
export COLORIZE_CHUNKS=1                    # Color-coded chunk display
export LOG_QUERIES=1                        # Save queries to query_logs/

# Advanced logging:
# export LOG_LEVEL=DEBUG                    # For troubleshooting
# export PYTHONUNBUFFERED=1                 # Disable output buffering

# ==========================================
# Paths
# ==========================================
export PDF_PATH=/workspace/data
export PGTABLE=messenger_runpod

# ==========================================
# Optional: HuggingFace Cache
# ==========================================
export HF_HOME=/workspace/huggingface_cache
export TRANSFORMERS_CACHE=/workspace/huggingface_cache

# ==========================================
# Usage
# ==========================================
# source runpod_config.env
# python3 rag_low_level_m1_16gb_verbose.py --query-only --query "your query"
