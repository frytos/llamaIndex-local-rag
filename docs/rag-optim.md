‚è∫ RAG Optimization Report

  Current Configuration

  Chunking & Retrieval
  - Chunk size: 700 characters, 150 overlap (21.4%)
  - Actual avg length: 1353 chars (93% over target)
  - TOP_K: 4 chunks

  Embedding
  - Model: BAAI/bge-small-en (384d)
  - Batch size: 64 (env default, code default is 32)
  - Backend: HuggingFace + MPS

  LLM
  - Model: Mistral 7B GGUF via llama.cpp
  - Context window: 3072 tokens
  - GPU layers: 24 (env default, code default is 16)
  - Batch size: 256 (env default, code default is 128)
  - Max tokens: 256

  System Resources
  - RAM: 16.0 GB total, 78% used (3.5 GB available)
  - MPS: Available (Apple Metal)
  - Swap: 83.5% used ‚ö†Ô∏è

  Database
  - Largest table: data_messages_text_cs100_ov20 (1.2 GB, ~100k chunks)
  - Current table: data_messages_text_cs700_ov150 (210 MB, ~40k chunks)

  ---
  Critical Issues Found

  üî¥ 1. High Memory Pressure (URGENT)

  Current: 78% RAM used, 83.5% swap used
  Impact: Severe performance degradation, system instability
  Risk: OOM crashes during LLM generation

  üü° 2. Chunk Size Mismatch

  Config: CHUNK_SIZE=700
  Reality: Average chunk length is 1353 chars (93% larger)
  Impact: Higher token consumption, reduced retrieval precision
  Cause: Likely preserving sentence boundaries

  üü° 3. Context Window Near Limit

  Current: 3072 tokens
  Typical usage: ~2400-2800 tokens (4 chunks √ó 1353 chars ‚âà 1200 tokens + prompt)
  Risk: Overflow on complex queries or longer chunks

  üü¢ 4. GPU Underutilization

  Current: N_GPU_LAYERS=24
  Possible: 28-32 layers (limited by memory pressure)
  Impact: Missing 10-15% generation speedup

  ---
  Recommendations

  üö® IMMEDIATE ACTIONS (Fix Memory Crisis)

  1. Reduce GPU Layers to Free RAM

  export N_GPU_LAYERS=20  # Down from 24
  Expected: Free ~800 MB RAM, reduce swap pressure
  Trade-off: ~10% slower generation, but system stability

  2. Close Unnecessary Applications

  Check and close memory-intensive apps:
  # Check top memory consumers
  ps aux | sort -k 4 -r | head -10

  3. Use Smaller Batch Sizes During Peak Load

  export N_BATCH=128      # Down from 256
  export EMBED_BATCH=32   # Down from 64
  Expected: Reduce peak memory by 500-800 MB

  ---
  ‚ö° QUICK WINS (High Impact, Low Effort)

  4. Increase Context Window (Once Memory Fixed)

  export CTX=4096  # Up from 3072
  Benefit: Eliminate overflow risk
  Cost: +512 MB RAM when fixed

  5. Align Chunk Size with Reality

  # Option A: Tighter chunks for precision
  export CHUNK_SIZE=500
  export CHUNK_OVERLAP=100

  # Option B: Accept larger chunks, increase CTX
  export CHUNK_SIZE=1400  # Match reality
  export CHUNK_OVERLAP=280  # 20%
  export CTX=6144

  6. Enable MLX Embeddings (5-20x Faster)

  pip install mlx
  export EMBED_BACKEND=mlx
  export EMBED_BATCH=128  # MLX handles larger batches
  Expected: 5-20x faster embedding (from 67 ‚Üí 335-1340 chunks/s)

  ---
  üìä QUALITY IMPROVEMENTS

  7. Optimize TOP_K Based on Context Budget

  # Conservative (fits in 3072 tokens)
  export TOP_K=3

  # Balanced (requires CTX=4096)
  export TOP_K=4  # Current

  # Comprehensive (requires CTX=6144)
  export TOP_K=5

  8. Consider Hybrid Search

  Add keyword matching for better recall:
  # Future enhancement: Implement BM25 + vector hybrid
  # Available in LlamaIndex via FusionRetriever

  ---
  üî¨ ADVANCED OPTIMIZATIONS

  9. Upgrade to Larger Embedding Model (When Memory Allows)

  export EMBED_MODEL=BAAI/bge-base-en-v1.5
  export EMBED_DIM=768
  Expected: +10-15% retrieval quality
  Cost: 2x slower embeddings, +30% storage

  10. Database Index Optimization

  -- Add HNSW index for faster retrieval
  CREATE INDEX IF NOT EXISTS idx_embedding_hnsw
  ON data_messages_text_cs700_ov150
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

  ---
  Recommended Configurations

  üè• Emergency Mode (Stabilize System Now)

  export CHUNK_SIZE=700
  export CHUNK_OVERLAP=150
  export TOP_K=3
  export N_GPU_LAYERS=20     # ‚¨áÔ∏è Reduce memory
  export N_BATCH=128         # ‚¨áÔ∏è Reduce memory
  export CTX=3072
  export EMBED_BATCH=32
  Goal: Reduce RAM usage, eliminate swap thrashing

  ‚öñÔ∏è Balanced Mode (After Memory Fixed)

  export CHUNK_SIZE=500
  export CHUNK_OVERLAP=100
  export TOP_K=4
  export N_GPU_LAYERS=24
  export N_BATCH=256
  export CTX=4096           # ‚¨ÜÔ∏è More headroom
  export EMBED_BATCH=64
  export EMBED_BACKEND=mlx  # ‚¨ÜÔ∏è 5-20x faster
  Goal: Better precision, good performance, safe margins

  üöÄ Performance Mode (If You Upgrade RAM to 32GB)

  export CHUNK_SIZE=500
  export CHUNK_OVERLAP=100
  export TOP_K=5
  export N_GPU_LAYERS=28    # ‚¨ÜÔ∏è More GPU offload
  export N_BATCH=512        # ‚¨ÜÔ∏è Larger batches
  export CTX=6144           # ‚¨ÜÔ∏è Bigger context
  export EMBED_BATCH=128
  export EMBED_BACKEND=mlx
  export EMBED_MODEL=BAAI/bge-base-en-v1.5
  export EMBED_DIM=768

  ---
  Expected Improvements

  Emergency ‚Üí Balanced

  - Memory: 78% ‚Üí 60% RAM usage, 84% ‚Üí 20% swap
  - Stability: High crash risk ‚Üí Stable
  - Speed: Embedding 67 ‚Üí 335+ chunks/s (+400%)
  - Quality: Similar (slight +5% from tighter chunks)

  Balanced ‚Üí Performance (with 32GB RAM)

  - Generation: 10 ‚Üí 13 tokens/s (+30%)
  - Quality: +15% retrieval precision
  - Context: 4096 ‚Üí 6144 tokens (+50%)

  ---
  Implementation Steps

  Step 1: Immediate Stabilization

  # Apply emergency config
  export N_GPU_LAYERS=20 N_BATCH=128 EMBED_BATCH=32

  # Test with a query
  python rag_low_level_m1_16gb_verbose.py --query-only --query "test"

  Step 2: Install MLX (Optional but Recommended)

  source .venv/bin/activate
  pip install mlx

  Step 3: Re-index with Optimized Settings

  # After memory stabilizes, use balanced config
  export CHUNK_SIZE=500 CHUNK_OVERLAP=100 TOP_K=4 CTX=4096
  export N_GPU_LAYERS=24 N_BATCH=256 EMBED_BATCH=64
  export EMBED_BACKEND=mlx  # If installed
  export PGTABLE=optimized_cs500_ov100
  export RESET_TABLE=1

  python rag_low_level_m1_16gb_verbose.py

  Step 4: Monitor Performance

  # Watch memory during operation
  watch -n 2 "ps aux | sort -k 4 -r | head -5"

  # Check query performance
  time python rag_low_level_m1_16gb_verbose.py --query-only \
    --query "What are the main topics discussed?"

  ---
  Monitoring Commands

  # Real-time memory monitoring
  source .venv/bin/activate && python3 -c "
  import psutil, time
  while True:
      mem = psutil.virtual_memory()
      swap = psutil.swap_memory()
      print(f'\rRAM: {mem.percent:.1f}% | Swap: {swap.percent:.1f}%', end='')
      time.sleep(1)
  "

  # GPU power usage
  sudo powermetrics --samplers gpu_power -i 1000 -n 5

  # Database query performance
  PGPASSWORD=frytos psql -h localhost -U fryt -d vector_db -c \
    "EXPLAIN ANALYZE SELECT * FROM data_messages_text_cs700_ov150
     ORDER BY embedding <=> '[0.1, 0.2, ...]' LIMIT 4;"

  ---

‚è∫ Summary

  Your RAG pipeline has critical memory pressure (83.5% swap usage) that's severely impacting performance. Here's what to do:

  RIGHT NOW:
  1. Reduce N_GPU_LAYERS to 20 and N_BATCH to 128 to free ~1GB RAM
  2. Close unnecessary applications

  NEXT:
  3. Install MLX for 5-20x faster embeddings: pip install mlx
  4. Increase CTX to 4096 for safety margin
  5. Re-index with CHUNK_SIZE=500 for better precision

  LATER (if you upgrade to 32GB RAM):
  6. Increase N_GPU_LAYERS to 28 for +30% generation speed
  7. Use larger embedding model for +15% quality

  The emergency configuration above will stabilize your system immediately. Once stable, the balanced mode will give you much better performance with MLX embeddings.
