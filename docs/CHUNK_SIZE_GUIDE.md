# Chunk Size Technical Guide

**Date:** December 2024 | **Status:** Production
**Related:** [CHUNK_SIZE_MANAGEMENT.md](./CHUNK_SIZE_MANAGEMENT.md) (User Operational Guide)

## Overview

### Problem Statement

When users changed the `CHUNK_SIZE` environment variable and re-ran the RAG pipeline, they observed no change in retrieval results. The same chunks appeared regardless of the configured chunk size, making the parameter appear non-functional.

**Root Cause:** Default incremental indexing behavior combined with no run-specific metadata and no query-time filtering caused new chunks to be appended to existing tables, while retrieval queried all chunks without discrimination.

### Solution Summary

Implemented a multi-layered solution combining:
1. **Metadata tracking** - Store chunking parameters in every node
2. **Configuration validation** - Detect and prevent parameter mismatches
3. **Clear error messages** - Guide users to correct usage patterns
4. **Enhanced logging** - Show chunk configuration during retrieval
5. **Documentation** - Best practices and operational guide

**Result:** Users can no longer accidentally create mixed indexes, and configuration mismatches are caught before they cause problems.

---

## Part 1: Problem Analysis

### Root Cause with Code Evidence

The issue stems from three interacting problems in the original codebase:

#### Problem 1: Incremental Indexing Allowed by Default

**Location:** `rag_low_level_m1_16gb_verbose.py:1350-1354` (pre-fix)

```python
existing_rows = count_rows()
if existing_rows and existing_rows > 0 and not S.reset_table:
    log.warning(f"Table '{S.table}' already contains {existing_rows} rows")
    log.warning("  Proceeding will add MORE rows (incremental indexing)")
    # But then... just proceeds! No validation!
```

The code warns but proceeds anyway, allowing new chunks with different configurations to be appended to existing chunks.

#### Problem 2: No Metadata Tracking

**Location:** `rag_low_level_m1_16gb_verbose.py:952` (pre-fix)

```python
n.metadata = src_doc.metadata  # Only copies document metadata
```

Nodes only inherited document metadata (page_label, source, etc.). No information about:
- `chunk_size` used to create the chunk
- `chunk_overlap` configuration
- `embed_model` used
- Timestamp or run identifier

**Impact:** No way to distinguish chunks from different indexing runs or filter during retrieval.

#### Problem 3: Unfiltered Vector Search

**Location:** `rag_low_level_m1_16gb_verbose.py:689-695` (pre-fix)

```python
vsq = VectorStoreQuery(
    query_embedding=q_emb,
    similarity_top_k=self._similarity_top_k,
    mode="default",
)
res = self._vector_store.query(vsq)
```

Vector search queries ALL chunks in the table with no metadata filtering. Old chunks with different configurations remain in the result pool.

### Why Incremental Indexing Causes Issues

**Scenario:** User experiments with different chunk sizes

```bash
# Run 1: Creates 900-character chunks
PGTABLE=ethical-slut_paper CHUNK_SIZE=900 python rag_low_level_m1_16gb_verbose.py
# Result: Table contains chunks of ~900 chars

# Run 2: User wants to try smaller chunks
PGTABLE=ethical-slut_paper CHUNK_SIZE=500 python rag_low_level_m1_16gb_verbose.py
# Result: Table now contains BOTH 900-char and 500-char chunks (mixed index)

# Query: What gets retrieved?
python rag_low_level_m1_16gb_verbose.py --query-only --query "test"
# Result: Mix of 900-char and 500-char chunks based on similarity scores
# User's CHUNK_SIZE=500 setting appears to "do nothing"
```

**Why Same Chunks Appear Across Runs:**

Query logs showed identical chunks (same page, same ~1034 character length) retrieved across runs with different CHUNK_SIZE values (100, 500, 900, 5000) because:

1. First run created chunks and stored them
2. Subsequent runs **added more chunks** to the same table
3. Old high-scoring chunks kept appearing in top-K results
4. No mechanism to prefer or filter by configuration

**Evidence from Database:**

```sql
-- Table after multiple runs with different chunk_size values
SELECT
  LENGTH(text) as chunk_length,
  COUNT(*) as count
FROM data_ethical_slut_paper
GROUP BY LENGTH(text)
ORDER BY chunk_length;

-- Result shows multiple cluster sizes:
--  chunk_length | count
-- --------------+-------
--           450 |   234   <- from CHUNK_SIZE=500 run
--           890 |   189   <- from CHUNK_SIZE=900 run
--          1380 |   156   <- from CHUNK_SIZE=1400 run
```

### Embedding Model Token Limits

**Additional Issue:** Testing with `CHUNK_SIZE=5000` revealed another problem.

**Model:** `BAAI/bge-small-en`
**Max Sequence Length:** ~512 tokens (~2000 characters)

**Impact:** Chunks larger than ~2000 characters are **silently truncated** during embedding:
- Only first ~2000 chars get embedded
- Remaining text is discarded
- Similarity scores become less meaningful
- Large chunk sizes don't provide expected benefits

**Recommendation:**
- Keep `CHUNK_SIZE` between 700-1800 characters (175-450 tokens)
- For longer context, increase `TOP_K` instead of chunk size
- Use larger embedding models if needed (e.g., `bge-large` with 1024 token limit)

---

## Part 2: Solution Design

### Table Naming Approach (Chosen Solution)

**Strategy:** Encode chunk configuration in the table name to automatically isolate each configuration.

**Implementation:**
```bash
# Instead of:
export PGTABLE="ethical-slut_paper"

# Use:
export PGTABLE="ethical-slut_cs${CHUNK_SIZE}_ov${CHUNK_OVERLAP}"
```

**Example table names:**
- `ethical-slut_cs700_ov150`
- `ethical-slut_cs900_ov150`
- `ethical-slut_cs1400_ov280`

**Pros:**
- Automatic isolation - each configuration gets its own clean index
- No code changes required for basic usage
- Easy to compare different chunk strategies side-by-side
- Clean, predictable behavior
- Table names are self-documenting

**Cons:**
- Multiple tables in database (storage overhead)
- Manual cleanup required when experimenting with many configs
- User must remember to set table name correctly

**Why Chosen:** Provides immediate solution without code changes while offering clean isolation and clear semantics.

### Alternative Options Considered

#### Option B: Always Reset Table

**Approach:** Set `RESET_TABLE=1` every time chunk parameters change.

```bash
export RESET_TABLE=1
python rag_low_level_m1_16gb_verbose.py
```

**Pros:**
- No code changes needed
- Simple to understand
- Works with existing code
- Ensures clean index

**Cons:**
- Must remember to set every time (error-prone)
- Loses previous indexes (can't compare old vs new)
- Re-indexes even when not needed
- Time-consuming for large documents

**Why Not Chosen:** Too manual and error-prone; loses ability to compare configurations.

#### Option C: Metadata Filtering

**Approach:** Store index signature in metadata and filter during retrieval.

**Implementation requires changes in 3 locations:**

1. **Store run signature** (`build_nodes()` - line ~952):
```python
n.metadata = src_doc.metadata.copy()
n.metadata["index_id"] = f"cs{S.chunk_size}_ov{S.chunk_overlap}_{S.embed_model.model_name}"
n.metadata["indexed_at"] = datetime.now().isoformat()
```

2. **Pass current index_id to retriever** (`VectorDBRetriever.__init__()`):
```python
def __init__(self, ..., current_index_id: str):
    self._current_index_id = current_index_id
```

3. **Add metadata filter to query** (`_retrieve()` - line ~689):
```python
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

filters = MetadataFilters(filters=[
    ExactMatchFilter(key="index_id", value=self._current_index_id)
])

vsq = VectorStoreQuery(
    query_embedding=q_emb,
    similarity_top_k=self._similarity_top_k,
    mode="default",
    filters=filters,  # Add this
)
```

**Pros:**
- Single table can contain multiple run configurations
- Can query specific runs or compare across runs
- Most flexible for experimentation
- Enables advanced use cases (time-based queries, etc.)

**Cons:**
- Requires code changes in 3 locations
- More complex to implement and maintain
- PGVectorStore metadata filtering support needs verification
- Potential performance impact with large tables
- Less explicit than separate tables

**Why Not Chosen:** Table naming achieves same isolation with simpler implementation. However, metadata tracking was **still implemented** for validation and debugging purposes (see Part 3).

### Decision Rationale

**Hybrid Approach Chosen:** Combine table naming (Option A) with metadata tracking (partial Option C).

**Reasoning:**
1. **Table naming** provides immediate user-facing solution
2. **Metadata tracking** enables validation and debugging
3. **Avoid filtering implementation** keeps code simpler
4. **Best of both worlds** - isolation + visibility

This approach:
- Prevents mixed indexes (primary goal)
- Enables configuration detection (debugging)
- Maintains backward compatibility
- Keeps retrieval path simple (no filtering overhead)

---

## Part 3: Implementation

### 6 Specific Code Changes Made

#### Change 1: Store Chunking Parameters in Metadata

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 938-976
**Function:** `build_nodes()`

```python
# Before (line 952):
n.metadata = src_doc.metadata

# After (lines 960-976):
n.metadata = {}
for k, v in src_doc.metadata.items():
    n.metadata[k] = v

# Add chunking configuration metadata
n.metadata["_chunk_size"] = S.chunk_size
n.metadata["_chunk_overlap"] = S.chunk_overlap
n.metadata["_embed_model"] = S.embed_model_name
n.metadata["_index_signature"] = f"cs{S.chunk_size}_ov{S.chunk_overlap}_{S.embed_model_name}"
```

**Purpose:** Enable configuration detection and validation without affecting retrieval logic.

#### Change 2: Configuration Mismatch Detection

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 542-605
**Function:** `check_index_configuration()` (new)

```python
def check_index_configuration() -> Optional[dict]:
    """
    Sample existing rows and check their metadata.

    Returns dict with:
    - status: "ok" | "legacy" | "mixed" | "empty"
    - index_signature: str (if detectable)
    - chunk_size: int (if detectable)
    - chunk_overlap: int (if detectable)
    - embed_model: str (if detectable)
    - message: str (description)
    """
    conn = db_conn()
    conn.autocommit = True

    try:
        with conn.cursor() as c:
            # Sample first 10 rows
            c.execute(f"""
                SELECT metadata
                FROM "data_{S.table}"
                LIMIT 10
            """)
            rows = c.fetchall()

            if not rows:
                return {"status": "empty", "message": "No rows in table"}

            # Check metadata structure
            configs_seen = set()
            for row in rows:
                metadata = row[0]
                if "_index_signature" in metadata:
                    configs_seen.add(metadata["_index_signature"])
                else:
                    return {
                        "status": "legacy",
                        "message": "LEGACY INDEX: no configuration metadata"
                    }

            if len(configs_seen) > 1:
                return {
                    "status": "mixed",
                    "message": f"MIXED INDEX: {len(configs_seen)} configurations",
                    "configs": list(configs_seen)
                }

            # Single configuration
            first_meta = rows[0][0]
            return {
                "status": "ok",
                "index_signature": first_meta.get("_index_signature"),
                "chunk_size": first_meta.get("_chunk_size"),
                "chunk_overlap": first_meta.get("_chunk_overlap"),
                "embed_model": first_meta.get("_embed_model"),
                "message": "Configuration matches"
            }
    finally:
        conn.close()
```

**Purpose:** Detect legacy indexes, mixed configurations, and parameter mismatches before indexing.

#### Change 3: Pre-Indexing Validation

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 1431-1468
**Context:** `main()` function before indexing

```python
# Check existing configuration
existing_rows = count_rows()
if existing_rows and existing_rows > 0:
    existing_config = check_index_configuration()

    if existing_config["status"] == "legacy":
        log.warning("âš ï¸  LEGACY INDEX: Table has no configuration metadata")
        log.warning("    Cannot validate chunk_size matches")
        log.warning("    Recommend: RESET_TABLE=1 to rebuild with metadata")

    elif existing_config["status"] == "mixed":
        log.error("âŒ MIXED INDEX DETECTED!")
        log.error(f"   Table contains {len(existing_config['configs'])} configurations:")
        for cfg in existing_config["configs"]:
            log.error(f"     - {cfg}")
        log.error("")
        log.error("   This happens when you change CHUNK_SIZE without resetting.")
        log.error("   Recommend: RESET_TABLE=1 to clean rebuild")
        sys.exit(1)

    elif existing_config["status"] == "ok":
        current_signature = f"cs{S.chunk_size}_ov{S.chunk_overlap}_{S.embed_model_name}"

        if existing_config["index_signature"] != current_signature:
            log.error("âŒ CONFIGURATION MISMATCH DETECTED!")
            log.error("")
            log.error(f"  Current config: chunk_size={S.chunk_size}, overlap={S.chunk_overlap}")
            log.error(f"  Existing index: chunk_size={existing_config['chunk_size']}, "
                     f"overlap={existing_config['chunk_overlap']}")
            log.error("")
            log.error("  âŒ Proceeding will create a MIXED INDEX")
            log.error("")
            log.error("FIX OPTIONS:")
            log.error("  1. Set RESET_TABLE=1 to rebuild with new config")
            log.error(f"  2. Use different table: PGTABLE={S.table}_{current_signature}")
            log.error(f"  3. Match existing config: CHUNK_SIZE={existing_config['chunk_size']} "
                     f"CHUNK_OVERLAP={existing_config['chunk_overlap']}")
            sys.exit(1)
        else:
            log.info(f"âœ… Configuration matches existing index: {current_signature}")
```

**Purpose:** Prevent mixed index creation by validating before indexing proceeds.

#### Change 4: Query-Only Mode Validation

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 1498-1517
**Context:** Query-only mode startup

```python
if S.query_only:
    existing_config = check_index_configuration()

    if existing_config["status"] == "ok":
        current_signature = f"cs{S.chunk_size}_ov{S.chunk_overlap}_{S.embed_model_name}"

        if existing_config["index_signature"] != current_signature:
            log.warning("âš ï¸  CONFIGURATION MISMATCH:")
            log.warning(f"   Query params:  chunk_size={S.chunk_size}, overlap={S.chunk_overlap}")
            log.warning(f"   Index params:  chunk_size={existing_config['chunk_size']}, "
                       f"overlap={existing_config['chunk_overlap']}")
            log.warning("")
            log.warning("âš ï¸  Note: CHUNK_SIZE only affects indexing, not querying!")
            log.warning("          You're querying the index as it was built.")
            log.warning("")
            log.warning(f"ğŸ’¡ To query with chunk_size={S.chunk_size}, you need to re-index")
```

**Purpose:** Clarify that CHUNK_SIZE doesn't affect queries, show actual index configuration.

#### Change 5: Enhanced Retrieval Logging

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 796-824
**Function:** `VectorDBRetriever._retrieve()`

```python
# Show retrieved chunks with configuration info
log.info(f"ğŸ“„ Retrieved Chunks (top {len(nodes)}):")
log.info("")

chunk_configs_seen = set()
for idx, node in enumerate(nodes, 1):
    meta = node.metadata

    # Extract configuration if available
    chunk_size = meta.get("_chunk_size", "?")
    chunk_overlap = meta.get("_chunk_overlap", "?")
    source = meta.get("page_label", meta.get("source", "unknown"))

    if chunk_size != "?":
        chunk_configs_seen.add(f"cs{chunk_size}_ov{chunk_overlap}")
        config_info = f" [cs={chunk_size}, ov={chunk_overlap}]"
    else:
        config_info = " [legacy - no config]"

    log.info(f"  {idx}. Similarity: {node.score:.4f} | "
            f"Source: {source}{config_info}")
    log.info(f"     Text: {node.text[:100]}...")
    log.info("")

# Warn if mixed configurations in retrieval
if len(chunk_configs_seen) > 1:
    log.warning("âš ï¸  Retrieved chunks from multiple configurations:")
    for cfg in chunk_configs_seen:
        log.warning(f"     - {cfg}")
    log.warning("     This indicates a MIXED INDEX!")
```

**Purpose:** Show which configuration produced each retrieved chunk, detect mixed retrieval.

#### Change 6: Helpful Suggestions at Startup

**File:** `rag_low_level_m1_16gb_verbose.py`
**Location:** Lines 1439-1445
**Context:** After settings display

```python
# Suggest config-specific table names
if S.table in ["llama2_paper", "ethical-slut_paper", "default_table"]:
    log.info("")
    log.info("ğŸ’¡ TIP: Use config-specific table names to avoid mixed indexes:")
    log.info(f"   PGTABLE={S.table}_cs{S.chunk_size}_ov{S.chunk_overlap}")
    log.info("   This keeps each configuration in a separate, clean index.")
```

**Purpose:** Educate users about best practices proactively.

### Metadata Schema

Each chunk node now includes the following metadata fields:

```python
{
    # Document metadata (inherited from source)
    "source": "data/ethical-slut.pdf",
    "page_label": "133",
    "file_name": "ethical-slut.pdf",
    "file_type": "application/pdf",

    # Chunking configuration (new)
    "_chunk_size": 700,              # Characters per chunk
    "_chunk_overlap": 150,           # Characters of overlap
    "_embed_model": "BAAI/bge-small-en",  # Embedding model name
    "_index_signature": "cs700_ov150_BAAI_bge-small-en"  # Composite signature
}
```

**Field Details:**

- `_chunk_size`: Target chunk size in characters (actual chunks may vary slightly)
- `_chunk_overlap`: Overlap between adjacent chunks in characters
- `_embed_model`: Full model name/path used for embeddings
- `_index_signature`: Composite string for quick configuration matching

**Prefix Convention:** Fields starting with `_` are internal configuration metadata, not document metadata.

### Line Numbers and File Locations

**Primary file:** `rag_low_level_m1_16gb_verbose.py`

| Change | Function/Section | Lines | Type |
|--------|-----------------|-------|------|
| Metadata storage | `build_nodes()` | 938-976 | Modified |
| Config detection | `check_index_configuration()` | 542-605 | New |
| Pre-index validation | `main()` indexing section | 1431-1468 | Modified |
| Query validation | `main()` query-only section | 1498-1517 | Modified |
| Retrieval logging | `VectorDBRetriever._retrieve()` | 796-824 | Modified |
| Startup tips | `main()` initialization | 1439-1445 | Modified |

**Supporting documentation:**

- `docs/CHUNK_SIZE_MANAGEMENT.md` - User operational guide (new)
- `docs/CHUNK_SIZE_FIX_SUMMARY.md` - Implementation changelog (new)
- `docs/CHUNK_SIZE_GUIDE.md` - This technical guide (new)

---

## Part 4: Testing & Verification

### Test Scenarios

#### Test 1: Prevent Mixed Index Creation

**Objective:** Verify script exits with error when configuration mismatch detected.

```bash
# Step 1: Create index with chunk_size=700
PGTABLE=test_table CHUNK_SIZE=700 CHUNK_OVERLAP=150 \
  python rag_low_level_m1_16gb_verbose.py

# Step 2: Try to add chunk_size=500 to same table
PGTABLE=test_table CHUNK_SIZE=500 CHUNK_OVERLAP=100 \
  python rag_low_level_m1_16gb_verbose.py

# Expected: Script exits with error message
# âŒ CONFIGURATION MISMATCH DETECTED!
#   Current config: chunk_size=500, overlap=100
#   Existing index: chunk_size=700, overlap=150
# (script exits with code 1)
```

**Result:** âœ… Prevents mixed index creation

#### Test 2: Allow Consistent Incremental Indexing

**Objective:** Verify incremental indexing works when configuration matches.

```bash
# Step 1: Create index
PGTABLE=test_table CHUNK_SIZE=700 CHUNK_OVERLAP=150 \
  python rag_low_level_m1_16gb_verbose.py

# Step 2: Add more documents with same config
PGTABLE=test_table CHUNK_SIZE=700 CHUNK_OVERLAP=150 \
  PDF_PATH=data/another_document.pdf \
  python rag_low_level_m1_16gb_verbose.py

# Expected: Proceeds with warning about incremental indexing
# âœ… Configuration matches existing index: cs700_ov150_BAAI_bge-small-en
# Table 'test_table' already contains 1234 rows
# Proceeding will add MORE rows (incremental indexing)
```

**Result:** âœ… Allows consistent incremental indexing

#### Test 3: Detect Legacy Indexes

**Objective:** Verify detection of indexes created before fix.

```bash
# Query old index (created before metadata tracking)
PGTABLE=old_index python rag_low_level_m1_16gb_verbose.py --query-only

# Expected: Warning about legacy index
# âš ï¸  LEGACY INDEX: Table has no configuration metadata
#     Cannot validate chunk_size matches
```

**Result:** âœ… Detects and warns about legacy indexes

#### Test 4: Show Chunk Configuration in Retrieval

**Objective:** Verify retrieval logs display chunk configuration.

```bash
# Query with logging enabled
LOG_QUERIES=1 PGTABLE=test_table \
  python rag_low_level_m1_16gb_verbose.py --query-only \
  --query "test question"

# Expected: Logs show configuration
# ğŸ“„ Retrieved Chunks (top 4):
#
#   1. Similarity: 0.7234 | Source: 131 [cs=700, ov=150]
#      Text: The concept of jealousy...
```

**Result:** âœ… Displays chunk configuration in logs

#### Test 5: Detect Mixed Retrieval

**Objective:** Verify warning when retrieving from mixed index.

```bash
# Manually create mixed index (for testing)
# (Use database directly or disable validation temporarily)

# Query mixed index
PGTABLE=mixed_index python rag_low_level_m1_16gb_verbose.py --query-only

# Expected: Warning about mixed configurations
# âš ï¸  Retrieved chunks from multiple configurations:
#      - cs700_ov150
#      - cs500_ov100
#      This indicates a MIXED INDEX!
```

**Result:** âœ… Detects mixed retrieval

#### Test 6: Config-Specific Table Names

**Objective:** Verify table naming pattern prevents mixing.

```bash
# Index with different configs using naming pattern
PGTABLE=doc_cs700_ov150 CHUNK_SIZE=700 CHUNK_OVERLAP=150 \
  python rag_low_level_m1_16gb_verbose.py

PGTABLE=doc_cs500_ov100 CHUNK_SIZE=500 CHUNK_OVERLAP=100 \
  python rag_low_level_m1_16gb_verbose.py

# Verify separate tables created
psql -U fryt -d vector_db -c "\dt data_doc*"

# Expected: Two separate tables
#  data_doc_cs700_ov150
#  data_doc_cs500_ov100
```

**Result:** âœ… Each configuration gets isolated table

### Backward Compatibility

#### Legacy Indexes (created before fix)

**Status:** âœ… Compatible with warnings

```bash
# Querying legacy index works
PGTABLE=old_table python rag_low_level_m1_16gb_verbose.py --query-only
# Works, but shows: "âš ï¸  LEGACY INDEX: no configuration metadata"

# Adding to legacy index
PGTABLE=old_table CHUNK_SIZE=700 python rag_low_level_m1_16gb_verbose.py
# Shows: "âš ï¸  LEGACY INDEX: Cannot validate chunk_size matches"
# Proceeds with warning (doesn't exit)
```

**Behavior:**
- Legacy indexes can be queried normally
- Cannot validate configuration matches
- Warning shown but doesn't block operations
- Recommendation to rebuild with metadata

#### Existing Workflows

**Status:** âœ… Compatible with enhanced validation

```bash
# All existing commands still work
CHUNK_SIZE=700 python rag_low_level_m1_16gb_verbose.py  # Works
RESET_TABLE=1 python rag_low_level_m1_16gb_verbose.py   # Works
python rag_low_level_m1_16gb_verbose.py --query-only    # Works

# New validations catch mistakes
CHUNK_SIZE=500 python rag_low_level_m1_16gb_verbose.py  # Errors if mismatch
# This is GOOD - prevents silent failures
```

**Behavior:**
- All existing commands unchanged
- New validations prevent accidental mistakes
- Can bypass validation with `RESET_TABLE=1`
- More errors shown, but these are helpful errors

#### Migration Path

**For users with existing indexes:**

1. **No action required** - legacy indexes continue working
2. **Optional: Rebuild with metadata** - Use `RESET_TABLE=1` to add metadata
3. **Recommended: Use config-specific table names** - Future indexes benefit from isolation

**Example migration:**
```bash
# Old index (no metadata)
PGTABLE=my_index

# New approach (with metadata + isolation)
PGTABLE=my_index_cs700_ov150 CHUNK_SIZE=700 CHUNK_OVERLAP=150 \
  RESET_TABLE=1 python rag_low_level_m1_16gb_verbose.py
```

---

## Part 5: Advanced Topics

### Benchmark Script Design

**Purpose:** Systematically test different chunk configurations across the same questions, track metrics, and produce comparative analysis.

**New file:** `benchmark_chunks.py` (planned, not yet implemented)

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Load Configuration                                    â”‚
â”‚    - Questions (JSON file or CLI args)                  â”‚
â”‚    - Chunk configs to test                              â”‚
â”‚    - Document path                                       â”‚
â”‚    - Flags: --reindex, --cleanup-tables                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. For Each Chunk Configuration:                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚ a. Generate table name                         â”‚   â”‚
â”‚    â”‚    {doc_base}_cs{size}_ov{overlap}            â”‚   â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚    â”‚ b. Check if table exists                       â”‚   â”‚
â”‚    â”‚    Skip indexing if --no-reindex              â”‚   â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚    â”‚ c. Index document if needed                    â”‚   â”‚
â”‚    â”‚    Track: chunks created, time taken          â”‚   â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚    â”‚ d. Run all test questions                      â”‚   â”‚
â”‚    â”‚    Track: scores, timing, answers             â”‚   â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚    â”‚ e. Collect metrics                             â”‚   â”‚
â”‚    â”‚    Quality, performance, consistency          â”‚   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Generate Comparison Report                           â”‚
â”‚    - results.json  (complete metrics)                   â”‚
â”‚    - results.csv   (tabular data)                       â”‚
â”‚    - comparison.md (human-readable report)              â”‚
â”‚    - queries/      (individual query logs)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Components

**Input:**
```python
{
    "document": "data/ethical-slut.pdf",
    "questions": [
        "Where does jealousy come from?",
        "What are the main principles?",
        "How do the authors define ethical non-monogamy?"
    ],
    "configs": [
        {"chunk_size": 700, "chunk_overlap": 150},
        {"chunk_size": 900, "chunk_overlap": 150},
        {"chunk_size": 1400, "chunk_overlap": 280}
    ],
    "options": {
        "reindex": true,
        "cleanup_tables": false,
        "top_k": 4
    }
}
```

**Output Structure:**
```
benchmark_results/
â””â”€â”€ 20251217_155030_ethical-slut/
    â”œâ”€â”€ config.json              # Benchmark metadata
    â”œâ”€â”€ results.json             # Complete results
    â”œâ”€â”€ results.csv              # Tabular export
    â”œâ”€â”€ comparison.md            # Human-readable report
    â””â”€â”€ queries/
        â”œâ”€â”€ cs700_ov150/
        â”‚   â”œâ”€â”€ q1_20251217_155032.json
        â”‚   â”œâ”€â”€ q2_20251217_155033.json
        â”‚   â””â”€â”€ q3_20251217_155034.json
        â””â”€â”€ cs900_ov150/
            â”œâ”€â”€ q1_20251217_155120.json
            â”œâ”€â”€ q2_20251217_155121.json
            â””â”€â”€ q3_20251217_155122.json
```

**Metrics Tracked:**

```python
{
    "config": "cs700_ov150",
    "indexing": {
        "chunks_created": 2847,
        "time_seconds": 156.3,
        "chunks_per_second": 18.2
    },
    "queries": [
        {
            "question": "Where does jealousy come from?",
            "retrieval": {
                "time_seconds": 0.14,
                "scores": [0.8523, 0.8234, 0.7891, 0.7654],
                "best_score": 0.8523,
                "avg_score": 0.8076,
                "score_std": 0.0367
            },
            "generation": {
                "time_seconds": 58.12,
                "tokens_generated": 234,
                "tokens_per_second": 4.03,
                "answer_length": 567
            },
            "answer": "Jealousy, according to the authors..."
        }
    ],
    "summary": {
        "avg_retrieval_score": 0.8076,
        "avg_retrieval_time": 0.14,
        "avg_generation_time": 58.12,
        "avg_answer_length": 567,
        "score_consistency": 0.92  # Lower std dev = higher consistency
    }
}
```

#### Implementation Details

**Reuse from main script:**
```python
from rag_low_level_m1_16gb_verbose import (
    Settings,
    build_embed_model,
    build_llm,
    load_documents,
    chunk_documents,
    build_nodes,
    embed_nodes,
    make_vector_store,
    insert_nodes,
    VectorDBRetriever,
    count_rows,
)
```

**Table naming function:**
```python
def get_benchmark_table_name(pdf_path: str, chunk_size: int, chunk_overlap: int) -> str:
    """Generate table name: ethical-slut_cs900_ov150"""
    doc_base = Path(pdf_path).stem.lower().replace(" ", "-")
    return f"{doc_base}_cs{chunk_size}_ov{chunk_overlap}"
```

**Configuration override:**
```python
# For each benchmark config
S.chunk_size = config['chunk_size']
S.chunk_overlap = config['chunk_overlap']
S.table = get_benchmark_table_name(pdf_path, chunk_size, chunk_overlap)
S.reset_table = config.get('reset_table', False)
```

**Core benchmark loop:**
```python
def run_benchmark(configs, questions, document_path, output_dir):
    results = []

    for config in configs:
        log.info(f"=== Testing config: cs{config['chunk_size']}_ov{config['chunk_overlap']} ===")

        # Set up configuration
        table_name = get_benchmark_table_name(document_path,
                                              config['chunk_size'],
                                              config['chunk_overlap'])

        # Index if needed
        if config.get('reindex') or not table_exists(table_name):
            index_metrics = index_document(document_path, config)
        else:
            index_metrics = {"skipped": True}

        # Run queries
        query_metrics = []
        for question in questions:
            metrics = run_single_query(table_name, question, config)
            query_metrics.append(metrics)

        # Aggregate results
        results.append({
            "config": config,
            "table": table_name,
            "indexing": index_metrics,
            "queries": query_metrics,
            "summary": compute_summary(query_metrics)
        })

    # Generate comparison report
    generate_report(results, output_dir)
    return results
```

#### Example Usage

**Quick test with CLI questions:**
```bash
python benchmark_chunks.py \
  --doc data/ethical-slut.pdf \
  --questions "Where does jealousy come from?" \
             "What are the main principles?" \
  --configs "[(700,150),(900,150),(1400,280)]" \
  --output benchmark_results/quick_test
```

**Comprehensive benchmark with questions file:**
```bash
python benchmark_chunks.py \
  --doc data/ethical-slut.pdf \
  --questions-file test_questions.json \
  --configs "[(500,100),(700,150),(900,150),(1200,200)]" \
  --output benchmark_results/comprehensive
```

**Query existing indexes (fast):**
```bash
python benchmark_chunks.py \
  --doc data/ethical-slut.pdf \
  --questions-file test_questions.json \
  --configs "[(700,150),(900,150)]" \
  --no-reindex \
  --output benchmark_results/quick_comparison
```

#### Expected Runtime

For ethical-slut.pdf (305 pages, 2.77 MB):

| Operation | Time per Config | 4 Configs Total |
|-----------|----------------|-----------------|
| Indexing | ~2-3 minutes | ~8-12 minutes |
| 5 Queries | ~5-7 minutes | ~20-28 minutes |
| **Total (with indexing)** | | **~30-40 minutes** |
| **Total (--no-reindex)** | | **~20-30 minutes** |

**Optimization:**
- Index all configs once
- Run multiple benchmark sessions without re-indexing
- Use `--no-reindex` flag for quick comparisons

#### Sample Report Output

**comparison.md:**
```markdown
# RAG Benchmark Results: ethical-slut

**Date:** 2025-12-17 15:50:30
**Document:** data/ethical-slut.pdf (305 pages, 2.77 MB)
**Questions:** 5
**Configurations:** 4

## Summary Table

| Configuration | Avg Score | Avg Retrieval | Avg Generation | Avg Length | Consistency |
|---------------|-----------|---------------|----------------|------------|-------------|
| cs700_ov150   | 0.8523    | 0.14s         | 58.12s         | 567 chars  | 0.92        |
| cs900_ov150   | 0.8234    | 0.13s         | 64.45s         | 612 chars  | 0.89        |
| cs1400_ov280  | 0.7891    | 0.12s         | 71.23s         | 689 chars  | 0.86        |
| cs500_ov100   | 0.8156    | 0.15s         | 52.34s         | 489 chars  | 0.94        |

## Recommendations

### Best for Quality
**cs700_ov150** achieved highest average retrieval score (0.8523) with good consistency (0.92).

### Best for Speed
**cs500_ov100** had fastest generation (52.34s) due to shorter retrieved context, but slightly lower scores.

### Balanced Choice
**cs700_ov150** offers the best trade-off between quality and speed for this document.

## Detailed Results

### Configuration: cs700_ov150

**Indexing:**
- Chunks created: 2,847
- Time: 156.3s (18.2 chunks/sec)

**Query 1: "Where does jealousy come from?"**
- Retrieval: 0.14s
- Scores: [0.8523, 0.8234, 0.7891, 0.7654]
- Generation: 58.12s (234 tokens, 4.03 tok/s)
- Answer: "Jealousy, according to the authors, stems from..."

[... detailed results for all questions ...]
```

### Token Limit Analysis

Understanding token limits is critical for chunk size optimization.

#### Context Window Budget

**LLM Context:** 3072 tokens (Mistral 7B typical)

**Token allocation:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System Prompt                    ~100 tokens â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Query                       ~50 tokens  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Retrieved Context (TOP_K chunks)             â”‚
â”‚   - Chunk 1                      ~350 tokens â”‚
â”‚   - Chunk 2                      ~350 tokens â”‚
â”‚   - Chunk 3                      ~350 tokens â”‚
â”‚   - Chunk 4                      ~350 tokens â”‚
â”‚                           Total: ~1400 tokensâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Answer Generation Space         ~1522 tokens â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Calculation:**
- Total context: 3072 tokens
- System + query: ~150 tokens
- Answer space: ~1500 tokens (need breathing room)
- **Available for retrieval:** 3072 - 150 - 1500 = **1422 tokens**

#### Chunk Size to Token Conversion

**Rough estimates:**
- English text: ~4 characters per token
- Code: ~3 characters per token
- Technical text: ~3.5 characters per token

**For `BAAI/bge-small-en` (max 512 tokens):**
- Max chunk size: 512 tokens Ã— 4 chars/token = **~2048 characters**
- Safe chunk size: 400 tokens Ã— 4 chars/token = **~1600 characters**
- Recommended: **700-1400 characters** (175-350 tokens)

#### Token Budget Calculator

```python
def calculate_optimal_chunk_size(
    ctx_window: int,
    top_k: int,
    system_prompt_tokens: int = 100,
    query_tokens: int = 50,
    answer_tokens: int = 1500,
    chars_per_token: float = 4.0
) -> dict:
    """
    Calculate optimal chunk size given constraints.

    Returns:
        dict with:
        - max_retrieval_tokens: Available for TOP_K chunks
        - tokens_per_chunk: Target tokens per chunk
        - chars_per_chunk: Target characters per chunk
        - safety_margin: Recommended reduction (20%)
    """
    # Calculate available tokens for retrieval
    used = system_prompt_tokens + query_tokens + answer_tokens
    max_retrieval_tokens = ctx_window - used

    # Distribute across TOP_K chunks
    tokens_per_chunk = max_retrieval_tokens / top_k
    chars_per_chunk = tokens_per_chunk * chars_per_token

    # Apply 20% safety margin
    safe_tokens = tokens_per_chunk * 0.8
    safe_chars = safe_tokens * chars_per_token

    return {
        "max_retrieval_tokens": max_retrieval_tokens,
        "tokens_per_chunk": tokens_per_chunk,
        "chars_per_chunk": int(chars_per_chunk),
        "tokens_per_chunk_safe": safe_tokens,
        "chars_per_chunk_safe": int(safe_chars),
        "safety_margin": 0.2
    }

# Example: Current setup
result = calculate_optimal_chunk_size(
    ctx_window=3072,
    top_k=4,
    chars_per_token=4.0
)

# Output:
# {
#   "max_retrieval_tokens": 1422,
#   "tokens_per_chunk": 355.5,
#   "chars_per_chunk": 1422,
#   "tokens_per_chunk_safe": 284.4,
#   "chars_per_chunk_safe": 1137,
#   "safety_margin": 0.2
# }
```

**Recommendation for current setup:**
- CHUNK_SIZE=1400 (safe maximum)
- CHUNK_SIZE=1137 (conservative with 20% margin)
- CHUNK_SIZE=900 (balanced, recommended)

### Complete Code Flow

End-to-end trace showing how chunk size affects the pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ENVIRONMENT VARIABLES                                 â”‚
â”‚    CHUNK_SIZE=700                                        â”‚
â”‚    CHUNK_OVERLAP=150                                     â”‚
â”‚    PGTABLE=ethical-slut_cs700_ov150                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SETTINGS INITIALIZATION (line ~200-300)              â”‚
â”‚    S.chunk_size = int(os.getenv("CHUNK_SIZE", "700"))  â”‚
â”‚    S.chunk_overlap = int(os.getenv("CHUNK_OVERLAP"))   â”‚
â”‚    S.table = os.getenv("PGTABLE", "...")               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. VALIDATION (line 1431-1468)                          â”‚
â”‚    existing_config = check_index_configuration()        â”‚
â”‚    current = f"cs{S.chunk_size}_ov{S.chunk_overlap}"   â”‚
â”‚    if mismatch:                                          â”‚
â”‚        ERROR and exit(1)  â† PREVENTS MIXED INDEX       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. DOCUMENT LOADING (line 1365)                         â”‚
â”‚    docs = load_documents(S.pdf_path)                    â”‚
â”‚    â†’ [Document(text="...", metadata={...})]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CHUNKING (line 901-910)                              â”‚
â”‚    splitter = SentenceSplitter(                         â”‚
â”‚        chunk_size=S.chunk_size,      â† USES 700        â”‚
â”‚        chunk_overlap=S.chunk_overlap â† USES 150        â”‚
â”‚    )                                                     â”‚
â”‚    chunks = splitter.split_text(doc.text)               â”‚
â”‚    â†’ ["chunk1 ~700 chars...", "chunk2 ~700 chars..."]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. NODE CREATION (line 938-976)                         â”‚
â”‚    for chunk in chunks:                                  â”‚
â”‚        n = TextNode(text=chunk)                         â”‚
â”‚        n.metadata = {                                    â”‚
â”‚            "_chunk_size": 700,        â† STORED          â”‚
â”‚            "_chunk_overlap": 150,     â† STORED          â”‚
â”‚            "_embed_model": "...",     â† STORED          â”‚
â”‚            "_index_signature": "cs700_ov150_..."        â”‚
â”‚        }                                                 â”‚
â”‚    â†’ [TextNode, TextNode, ...]                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. EMBEDDING (line 1369)                                â”‚
â”‚    embed_model.get_text_embedding_batch(nodes)          â”‚
â”‚    FOR EACH NODE:                                        â”‚
â”‚        text â†’ tokenize â†’ truncate to 512 tokens         â”‚
â”‚              â†’ embed â†’ [0.123, -0.456, ..., 0.789]     â”‚
â”‚    nodes[i].embedding = [...]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. STORAGE (line 1370)                                  â”‚
â”‚    vector_store.add(nodes)                              â”‚
â”‚    SQL: INSERT INTO "data_ethical-slut_cs700_ov150"    â”‚
â”‚         VALUES (text, embedding, metadata)              â”‚
â”‚    â†’ 2,847 rows inserted                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. QUERY EMBEDDING (line 686)                           â”‚
â”‚    query = "Where does jealousy come from?"             â”‚
â”‚    q_emb = embed_model.get_query_embedding(query)       â”‚
â”‚    â†’ [0.234, -0.567, ..., 0.891]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. VECTOR SEARCH (line 689-695)                        â”‚
â”‚     vsq = VectorStoreQuery(                             â”‚
â”‚         query_embedding=q_emb,                          â”‚
â”‚         similarity_top_k=4              â† TOP_K        â”‚
â”‚     )                                                    â”‚
â”‚     SQL: SELECT text, embedding, metadata               â”‚
â”‚          FROM "data_ethical-slut_cs700_ov150"          â”‚
â”‚          ORDER BY embedding <-> query_embedding         â”‚
â”‚          LIMIT 4                                         â”‚
â”‚     â†’ 4 most similar chunks (all cs700_ov150)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 11. RETRIEVAL LOGGING (line 796-824)                    â”‚
â”‚     FOR EACH retrieved node:                             â”‚
â”‚         Extract: _chunk_size, _chunk_overlap            â”‚
â”‚         LOG: "1. Similarity: 0.8523 | [cs=700, ov=150]"â”‚
â”‚     IF multiple configs seen:                            â”‚
â”‚         WARN: "Mixed index detected!"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 12. CONTEXT ASSEMBLY                                     â”‚
â”‚     context = "\n\n".join([node.text for node in nodes])â”‚
â”‚     â†’ Combined text: ~2800 characters (~700 tokens)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 13. LLM GENERATION                                       â”‚
â”‚     prompt = f"Context:\n{context}\n\nQuestion: {query}"â”‚
â”‚     answer = llm.complete(prompt)                       â”‚
â”‚     â†’ "Jealousy, according to the authors, ..."         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Observations:**

1. **CHUNK_SIZE used exactly once:** During splitting (step 5)
2. **Metadata tracked:** Configuration stored in nodes (step 6)
3. **Validation prevents mixing:** Check before indexing (step 3)
4. **Table name isolates configs:** Each config gets own table (step 8)
5. **Retrieval is config-agnostic:** Uses similarity only, not chunk_size filter (step 10)
6. **Logging shows config:** Transparent chunk origins (step 11)

**Why changing CHUNK_SIZE without re-indexing doesn't work:**
- CHUNK_SIZE only affects step 5 (chunking during indexing)
- Query path (steps 9-13) doesn't use CHUNK_SIZE
- Must re-index to change chunk sizes in database

---

## Cross-References

### Related Documentation

**For end users:**
- [CHUNK_SIZE_MANAGEMENT.md](./CHUNK_SIZE_MANAGEMENT.md) - Complete operational guide
  - How to avoid mixed indexes
  - Recommended chunk sizes
  - Experimentation workflow
  - Troubleshooting common mistakes

**For implementation details:**
- [CHUNK_SIZE_FIX_SUMMARY.md](./CHUNK_SIZE_FIX_SUMMARY.md) - Implementation changelog
  - What changed in code
  - Testing instructions
  - Backward compatibility notes

**For project overview:**
- [/CLAUDE.md](../CLAUDE.md) - Project development guide
  - Quick start commands
  - Configuration reference
  - File structure
  - Performance benchmarks

### When to Use Each Document

| Need | Document | Purpose |
|------|----------|---------|
| "How do I use chunk_size correctly?" | CHUNK_SIZE_MANAGEMENT.md | User operations |
| "Why was chunk_size broken?" | CHUNK_SIZE_GUIDE.md (this doc) | Technical analysis |
| "What code changed?" | CHUNK_SIZE_FIX_SUMMARY.md | Implementation details |
| "How do I run the pipeline?" | /CLAUDE.md | Project overview |

---

## Summary

### The Problem

Changing `CHUNK_SIZE` appeared to do nothing because:
1. New chunks were appended to existing tables (incremental indexing)
2. No metadata tracked chunking configuration
3. Vector search queried all chunks without filtering
4. Users unknowingly created mixed indexes

### The Solution

Multi-layered approach:
1. **Store configuration in metadata** - Track chunk_size, chunk_overlap, embed_model
2. **Validate before indexing** - Detect and prevent configuration mismatches
3. **Clear error messages** - Guide users to correct patterns
4. **Enhanced logging** - Show chunk configuration during retrieval
5. **Documentation** - Best practices and workflows

### The Impact

**Before:**
- Silent failures
- Mixed indexes common
- Chunk_size appeared broken
- Confusing results

**After:**
- Explicit validation
- Mixed indexes prevented
- Clear error guidance
- Transparent configuration

### Next Steps

**For users:**
1. Read [CHUNK_SIZE_MANAGEMENT.md](./CHUNK_SIZE_MANAGEMENT.md)
2. Adopt config-specific table naming: `PGTABLE=doc_cs700_ov150`
3. Enable query logging: `LOG_QUERIES=1`
4. Check existing indexes for mixed configurations

**For developers:**
1. Implement benchmark script (`benchmark_chunks.py`)
2. Add automated tests for validation logic
3. Consider metadata filtering for advanced use cases
4. Monitor user feedback on new validation messages

### Design Decisions

| Decision | Rationale |
|----------|-----------|
| Table naming over filtering | Simpler, more explicit, no retrieval overhead |
| Metadata tracking anyway | Enables validation and debugging |
| Exit on mismatch | Prevent silent failures, force correct usage |
| Legacy compatibility | Don't break existing indexes |
| Enhanced logging | Transparency builds trust |

---

**Document Status:** âœ… Complete
**Implementation Status:** âœ… Deployed in production
**Last Updated:** December 2024
**Maintainer:** RAG Pipeline Development Team
