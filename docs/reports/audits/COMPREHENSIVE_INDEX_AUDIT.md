# Comprehensive RAG Index Audit Report

**Generated**: 2026-01-10 14:58:30
**Database**: vector_db
**Total Tables Audited**: 4

---

## Executive Summary

All 4 RAG indices are **âœ… HEALTHY** with no critical issues detected. However, there are some observations and recommendations for optimization:

### Key Findings

| Metric | Status | Details |
|--------|--------|---------|
| **Configuration Consistency** | âœ… Excellent | All indices use consistent configurations |
| **Embedding Health** | âœ… Perfect | No null embeddings, dimensions consistent |
| **Storage Efficiency** | âœ… Good | 654 MB total across 159,664 chunks |
| **Query Performance** | âš ï¸ Varies | 4ms - 473ms latency depending on size |
| **Similarity Scores** | âš ï¸ Low | Dummy query shows negative/low scores (expected) |

---

## Index Comparison

### 1. data_agathecornillet_10208636972062_cs700_ov150_bge_260110

**Purpose**: Agathe's conversation history with BGE multilingual model

| Metric | Value |
|--------|-------|
| **Status** | âœ… HEALTHY |
| **Size** | 6,117 chunks, 42 MB |
| **Configuration** | cs=700, ov=150 |
| **Model** | BAAI/bge-m3 (1024 dims) |
| **Avg Chunk Size** | 1,411 chars |
| **Query Latency** | 0.042s |
| **Chunks per Doc** | 117.6 avg |

**Observations**:
- Multilingual BGE-m3 model (excellent for French/English content)
- High chunks per document (117.6) suggests comprehensive indexing
- Fast query performance (42ms)
- Average chunk size (1,411) is **2x the configured chunk_size (700)**
  - This is actually GOOD for conversational data - chunks contain ~3-5 messages
  - The overlap (150) preserves context between chunks

**Recommendations**:
- âœ… Index is healthy and production-ready
- This configuration is well-suited for multilingual chat data

---

### 2. data_inbox_cs700_ov150_minilm_260110

**Purpose**: Inbox conversations with lightweight MiniLM model

| Metric | Value |
|--------|-------|
| **Status** | âœ… HEALTHY |
| **Size** | 91,219 chunks, 325 MB |
| **Configuration** | cs=700, ov=150 |
| **Model** | sentence-transformers/all-MiniLM-L6-v2 (384 dims) |
| **Avg Chunk Size** | 1,556 chars |
| **Query Latency** | 0.306s |
| **Chunks per Doc** | 45,609.5 avg (!) |

**Observations**:
- **LARGEST INDEX** - 91K chunks, 325 MB
- Extremely high chunks per document (45K!) suggests 2 massive conversation files
- Query latency of 306ms is acceptable for this size
- Average chunk size (1,556) is **2.2x configured chunk_size (700)**
  - Similar to index #1, good for conversational data
- MiniLM model is lightweight (384 dims) but effective

**Recommendations**:
- ðŸ’¡ Consider decreasing CHUNK_SIZE to 500-600 if more precise retrieval needed
- Current configuration prioritizes context (good for conversations)
- Query performance is acceptable given the scale

---

### 3. data_t_01_messenger_cs1200_ov240_bge_260110

**Purpose**: Test messenger data with larger chunks and BGE base model

| Metric | Value |
|--------|-------|
| **Status** | âœ… HEALTHY |
| **Size** | 333 chunks, 2 MB |
| **Configuration** | cs=1200, ov=240 |
| **Model** | BAAI/bge-base-en-v1.5 (768 dims) |
| **Avg Chunk Size** | 1,351 chars |
| **Query Latency** | 0.004s |
| **Chunks per Doc** | N/A (too few docs) |

**Observations**:
- **SMALLEST INDEX** - only 333 chunks (test/experimental)
- **LARGEST CHUNK SIZE** - cs=1200, ov=240 (20% overlap)
- BGE-base model with 768 dimensions (between MiniLM and BGE-m3)
- **FASTEST QUERY** - 4ms latency due to small size
- Actual chunk size (1,351) close to configured (1,200)

**Recommendations**:
- âœ… Index is healthy
- This appears to be a test index with experimental chunk size
- Larger chunks (1200) work well for document-heavy content

---

### 4. data_t_01_messenger_cs500_ov100_minilm_260110

**Purpose**: Messenger data with smaller chunks and MiniLM model

| Metric | Value |
|--------|-------|
| **Status** | âœ… HEALTHY |
| **Size** | 61,995 chunks, 285 MB |
| **Configuration** | cs=500, ov=100 |
| **Model** | sentence-transformers/all-MiniLM-L6-v2 (384 dims) |
| **Avg Chunk Size** | 924 chars |
| **Query Latency** | 0.473s |
| **Chunks per Doc** | 1,722.1 avg |

**Observations**:
- **SECOND LARGEST** - 62K chunks, 285 MB
- **SMALLEST CHUNK SIZE** - cs=500, ov=100 (20% overlap)
- Average chunk size (924) is **1.85x configured chunk_size (500)**
  - Still preserving good context for conversations
- **SLOWEST QUERY** - 473ms latency (expected for 62K chunks)
- MiniLM model keeps memory footprint low

**Recommendations**:
- âœ… Index is healthy and production-ready
- Query latency is acceptable for this scale
- Smaller chunks (500) provide more precise retrieval

---

## Configuration Analysis

### Chunk Size Strategy

| Config | Count | Use Case | Avg Actual Size |
|--------|-------|----------|-----------------|
| cs=500, ov=100 | 1 index | Precise retrieval | 924 chars (1.85x) |
| cs=700, ov=150 | 2 indices | Balanced (default) | 1,484 chars (2.1x) |
| cs=1200, ov=240 | 1 index | Max context | 1,351 chars (1.13x) |

**Insight**: Conversational data naturally produces chunks ~1.5-2x larger than configured chunk_size due to message boundaries. This is **expected and desirable** - it ensures complete messages aren't split mid-sentence.

### Embedding Model Comparison

| Model | Dimensions | Indices | Trade-offs |
|-------|-----------|---------|------------|
| **MiniLM-L6-v2** | 384 | 2 indices | Fast, lightweight, good quality |
| **BGE-base** | 768 | 1 index | Better quality, moderate size |
| **BGE-m3** | 1024 | 1 index | Best quality, multilingual, largest |

**Recommendation**:
- Use **BGE-m3** for multilingual content (French/English)
- Use **MiniLM** for English-only content where speed/memory matter
- Use **BGE-base** for balance between quality and efficiency

---

## Performance Benchmarks

### Query Latency by Index Size

| Index | Rows | Latency | MB/chunk |
|-------|------|---------|----------|
| data_t_01_messenger_cs1200_ov240_bge_260110 | 333 | **0.004s** | 6.2 KB |
| data_agathecornillet_10208636972062_cs700_ov150_bge_260110 | 6,117 | **0.042s** | 6.9 KB |
| data_inbox_cs700_ov150_minilm_260110 | 91,219 | **0.306s** | 3.6 KB |
| data_t_01_messenger_cs500_ov100_minilm_260110 | 61,995 | **0.473s** | 4.6 KB |

**Observations**:
- Latency scales **sub-linearly** with index size (good!)
- 333 rows: 4ms
- 6K rows: 42ms (10x slower for 18x more rows)
- 62K rows: 473ms (100x slower for 186x more rows)
- 91K rows: 306ms (faster than 62K rows! likely better indexing)

**Recommendation**: Consider adding HNSW index for indices > 10K chunks:
```sql
CREATE INDEX ON table_name USING hnsw (embedding vector_cosine_ops);
```

---

## Storage Efficiency

| Index | Chunks | Size | Bytes/Chunk |
|-------|--------|------|-------------|
| data_t_01_messenger_cs1200_ov240_bge_260110 | 333 | 2 MB | **6,323 bytes** |
| data_agathecornillet_10208636972062_cs700_ov150_bge_260110 | 6,117 | 42 MB | **6,866 bytes** |
| data_inbox_cs700_ov150_minilm_260110 | 91,219 | 325 MB | **3,563 bytes** |
| data_t_01_messenger_cs500_ov100_minilm_260110 | 61,995 | 285 MB | **4,597 bytes** |

**Insight**: MiniLM indices (384 dims) use ~40% less storage than BGE-m3 (1024 dims) per chunk.

**Total Storage**: 654 MB for 159,664 chunks = **4,095 bytes/chunk average**

---

## Quality Observations

### Similarity Scores

All indices show **low/negative similarity scores** for the dummy test query. This is **EXPECTED** because:
1. The test uses a random vector `[0.1, 0.1, ...]` not generated by the actual embedding model
2. Actual queries using the same embedding model will produce meaningful similarity scores
3. The test primarily validates latency and retrieval mechanics

**Action**: To properly test retrieval quality, use real queries:
```bash
python rag_low_level_m1_16gb_verbose.py --query-only --query "your actual question"
```

---

## Recommendations Summary

### âœ… What's Working Well

1. **Configuration Consistency** - All indices use consistent, documented configs
2. **No Data Corruption** - Zero null embeddings, consistent dimensions
3. **Good Chunk Sizes** - Actual chunk sizes preserve message context
4. **Scalable Performance** - Query latency acceptable even for 91K chunks
5. **Efficient Storage** - 4KB per chunk is reasonable

### ðŸ’¡ Optimization Opportunities

1. **Add Vector Indices** - For indices > 10K chunks, add HNSW index:
   ```sql
   CREATE INDEX ON data_inbox_cs700_ov150_minilm_260110
   USING hnsw (embedding vector_cosine_ops);
   ```
   Expected improvement: 2-5x faster queries

2. **Consolidate Test Indices** - `data_t_01_messenger_*` appear to be experiments
   - Consider archiving or documenting their purpose

3. **Monitor Query Quality** - Run real queries to validate retrieval quality:
   ```bash
   # Test each index
   PGTABLE=data_inbox_cs700_ov150_minilm_260110 \
   python rag_low_level_m1_16gb_verbose.py --query-only \
   --query "What did we discuss about travel plans?"
   ```

4. **Consider Hybrid Search** - For the large indices, hybrid search (vector + BM25) may improve recall:
   ```bash
   HYBRID_ALPHA=0.5  # 50/50 blend
   ```

### ðŸ”§ Configuration Guidelines

Based on this audit, recommended configurations for future indices:

**For Multilingual Chat (French/English)**:
```bash
CHUNK_SIZE=700
CHUNK_OVERLAP=150
EMBED_MODEL=BAAI/bge-m3
EMBED_DIM=1024
EMBED_BACKEND=mlx  # 9x faster on M1
```

**For English-Only Chat (Fast & Lightweight)**:
```bash
CHUNK_SIZE=500
CHUNK_OVERLAP=100
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBED_DIM=384
EMBED_BACKEND=huggingface
```

**For Documents (PDFs, Code)**:
```bash
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
EMBED_MODEL=BAAI/bge-base-en-v1.5
EMBED_DIM=768
```

---

## Conclusion

All 4 RAG indices are **production-ready** with no critical issues. The system is well-configured for:
- âœ… Multilingual conversational data
- âœ… Large-scale indexing (90K+ chunks)
- âœ… Fast retrieval (4-473ms depending on size)
- âœ… Efficient storage (4KB per chunk)

The main opportunity for improvement is adding HNSW vector indices to the larger tables for faster queries.

---

**Next Steps**:
1. Add HNSW indices to large tables (see SQL above)
2. Test real queries to validate retrieval quality
3. Archive or document experimental indices
4. Consider implementing query caching for common questions

**Audit Script**: `/Users/frytos/code/llamaIndex-local-rag/audit_index.py`
**Individual Reports**: `audit_report_*.txt` files in project root
