# üîß Fix llama-cpp-python CUDA Support

**Issue:** LLM runs on CPU instead of GPU (5-10x slower)
**Status:** ‚úÖ Fixed in repo (commit 90c0cc2)

---

## üö® Symptom

### In Logs:
```
‚ùå load_tensors: layer 0 assigned to device CPU
‚ùå load_tensors: layer 1 assigned to device CPU
...
‚ùå llama_kv_cache_unified: layer 0: dev = CPU
```

### In nvidia-smi:
```
GPU Memory: 626 MiB (only embedding model)
GPU-Util: 0%
Processes: NONE
```

### Performance:
- Query time: ~40-60s (slow, CPU-bound)
- Expected: ~5-10s with GPU

---

## ‚úÖ Root Cause

**llama-cpp-python from pip** = CPU-only build (no CUDA compiled)

To use GPU, must compile from source with:
```bash
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

---

## üîß Fix for CURRENT Pod

### Quick Fix (While Pod is Running):

```bash
cd /workspace/rag-pipeline
source .venv/bin/activate

# Uninstall CPU version
pip uninstall -y llama-cpp-python

# Reinstall with CUDA (~3-5 minutes compile time)
CMAKE_ARGS="-DGGML_CUDA=on" \
FORCE_CMAKE=1 \
pip install llama-cpp-python==0.3.16 --force-reinstall --no-cache-dir --verbose
```

**Wait for:**
```
-- Found CUDA: /usr/local/cuda (found version "12.4")
-- GGML_CUDA: ON
...
Building llama.cpp with CUDA support...
...
Successfully installed llama-cpp-python-0.3.16
```

### Verify Fix:

```bash
# Test query again
export PGTABLE=messenger_gpu
export PDF_PATH=data/messenger_clean_small
export EMBED_BACKEND=torch
export N_GPU_LAYERS=99

time python3 rag_low_level_m1_16gb_verbose.py --query-only \
  --query "when did I go to New York"
```

**Look for in logs:**
```
‚úÖ llama_model_load: using device CUDA
‚úÖ llama_kv_cache_unified: layer 0: dev = CUDA
...
‚úÖ Generation: ~3-5s (fast!)
real    0m 8-10s
```

**And in nvidia-smi:**
```
Processes:
  python3    4-8 GB VRAM    50-80% GPU-Util
```

---

## üöÄ Fix for FUTURE Pods (Automatic)

### Code is Already Fixed!

The startup script now automatically:
1. Installs requirements.txt (includes CPU llama-cpp-python)
2. **Uninstalls CPU version**
3. **Reinstalls with CUDA support**
4. Verifies GPU acceleration

### New Pod Startup Timeline:

```
[0-5 min]  Install requirements.txt
[5-10 min] Compile llama-cpp-python with CUDA  ‚Üê NEW STEP
[10-12 min] Setup PostgreSQL + pgvector
[12-13 min] Test GPU + Download models

Total: ~12-15 minutes (vs ~7 minutes before)
```

**Trade-off:** +5 minutes setup for 5-10x faster queries!

---

## üìä Performance Comparison

| Setup | Query Time | LLM Speed | vs M1 |
|-------|-----------|-----------|-------|
| **M1 Metal** | ~65s | ~10 tok/s | 1x (baseline) |
| **RTX 4090 CPU** ‚ùå | ~40s | ~5 tok/s | 1.6x |
| **RTX 4090 GPU** ‚úÖ | **~8-10s** | **~50-100 tok/s** | **6-8x** ‚ö° |

---

## üéØ When to Apply This Fix

### Current Pod:
- Apply manually (commands above)
- Takes ~5 minutes
- Immediate speedup

### Future Pods:
- Automatic (code fixed on GitHub)
- Deploy with usual command
- LLM will use GPU from start

---

## üêõ Troubleshooting

### If Compilation Fails:

**Error:** `CMake not found`
```bash
apt update
apt install -y cmake
```

**Error:** `CUDA not found`
```bash
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

**Error:** `gcc not found`
```bash
apt install -y build-essential
```

### If Still Uses CPU After Fix:

Check llama.cpp was compiled with CUDA:
```bash
python3 -c "from llama_cpp import llama_cpp; print(dir(llama_cpp))" | grep -i cuda
```

Should see CUDA-related functions.

---

## üìù Summary

**Problem:** LLM on CPU (slow)
**Solution:** Compile llama-cpp-python with CUDA
**Result:** 5-10x faster queries
**Status:** ‚úÖ Fixed in repo (commit 90c0cc2)

**For current pod:** Run manual fix (5 min)
**For future pods:** Automatic (deploy as usual)

---

**Next:** Apply manual fix on current pod, then enjoy GPU-accelerated queries! üöÄ
