# Local RAG Pipeline - Environment Variables Template
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# SECURITY: Never commit .env to git! It's in .gitignore.

# ==========================================
# Database Configuration (Required)
# ==========================================
PGHOST=localhost
PGPORT=5432
PGUSER=your_database_user
PGPASSWORD=your_secure_password_here
DB_NAME=vector_db

# ==========================================
# Document & Table Configuration
# ==========================================
PDF_PATH=data/llama2.pdf          # Path to document(s) to index
PGTABLE=my_index                  # Table name (auto-generated if not set)
RESET_TABLE=0                     # 1 = drop table before indexing
RESET_DB=0                        # 1 = reset entire database (dangerous!)

# ==========================================
# Chunking Configuration (RAG Quality)
# ==========================================
CHUNK_SIZE=700                    # Characters per chunk (100-2000)
CHUNK_OVERLAP=150                 # Overlap between chunks (15-25% recommended)

# ==========================================
# Embedding Model Configuration
# ==========================================
EMBED_MODEL=BAAI/bge-small-en     # HuggingFace model name
EMBED_DIM=384                     # Vector dimensions
EMBED_BATCH=64                    # Batch size for embedding
EMBED_BACKEND=mlx                 # "mlx" (M1 Mac) or "huggingface"

# ==========================================
# LLM Configuration
# ==========================================
MODEL_URL=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
CTX=8192                          # Context window (3072-16384)
N_GPU_LAYERS=24                   # GPU layers (0=CPU, 24=balanced, 32=max)
N_BATCH=256                       # Batch size for LLM
MAX_NEW_TOKENS=256                # Max tokens to generate
TEMPERATURE=0.1                   # Temperature (0.0-1.0, use 0.0-0.3 for RAG)

# ==========================================
# Retrieval Configuration
# ==========================================
TOP_K=4                           # Number of chunks to retrieve
HYBRID_ALPHA=0.7                  # 0.0=pure vector, 1.0=pure BM25, 0.5=balanced
ENABLE_FILTERS=1                  # Enable metadata filtering (1=yes, 0=no)
MMR_THRESHOLD=0.5                 # MMR diversity threshold (0.0-1.0)

# ==========================================
# vLLM Configuration (Optional - for GPU acceleration)
# ==========================================
# USE_VLLM=1                      # Uncomment to use vLLM
# VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ
# VLLM_API_BASE=http://localhost:8000/v1
# VLLM_PORT=8000

# ==========================================
# Logging & Debugging
# ==========================================
LOG_LEVEL=INFO                    # DEBUG, INFO, WARNING, ERROR
LOG_FULL_CHUNKS=0                 # 1 = log full chunk content (verbose)
COLORIZE_CHUNKS=1                 # 1 = colorize output
SAVE_QUERY_LOG=1                  # 1 = save query history to JSON

# ==========================================
# Performance Tuning (Advanced)
# ==========================================
DB_INSERT_BATCH=250               # Database insert batch size
EXTRACT_CHAT_METADATA=0           # 1 = extract Facebook Messenger metadata
