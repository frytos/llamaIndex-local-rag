# ============================================================================
# Local RAG Pipeline - Comprehensive Environment Variables Template
# ============================================================================
#
# SETUP:
#   1. Copy this file to .env:
#      cp config/.env.example .env
#   2. Fill in your values (especially PGUSER and PGPASSWORD)
#   3. Load it: source .env  OR  export $(cat .env | xargs)
#
# SECURITY WARNING:
#   Never commit .env to git! It contains sensitive credentials.
#   The .env file is already in .gitignore.
#
# ============================================================================


# ============================================================================
# 1. DATABASE CONFIGURATION
# ============================================================================
# PostgreSQL connection settings (REQUIRED)

# Database host (localhost for local, IP/hostname for remote)
PGHOST=localhost

# PostgreSQL port (default: 5432)
PGPORT=5432

# Database username (REQUIRED - set your PostgreSQL user)
PGUSER=your_database_user

# Database password (REQUIRED - set your PostgreSQL password)
PGPASSWORD=your_secure_password_here

# Database name (will be auto-created if it doesn't exist)
DB_NAME=vector_db


# ============================================================================
# 2. DOCUMENT PROCESSING
# ============================================================================
# Source documents and index configuration

# Path to document(s) to index (file or folder)
# Supported: .pdf, .docx, .pptx, .html, .json, .txt, .md, .py, .cpp, etc.
PDF_PATH=data/llama2.pdf

# PostgreSQL table name for this index
# Leave unset for auto-generation based on config (recommended)
# Format: {filename}_cs{chunk_size}_ov{chunk_overlap}_{model_short}
# PGTABLE=my_custom_index

# Drop table before indexing (DESTRUCTIVE!)
# 0 = keep existing data, 1 = reset table
# Use 1 when changing chunk size/overlap to avoid mixed configs
RESET_TABLE=0

# Reset entire database (VERY DESTRUCTIVE!)
# 0 = normal operation, 1 = drop and recreate database
# WARNING: This deletes ALL tables in the database!
RESET_DB=0


# ============================================================================
# 3. CHUNKING CONFIGURATION (RAG Quality Tuning)
# ============================================================================
# Controls how documents are split into chunks for retrieval

# Chunk size in characters (100-2000)
# Guidelines:
#   100-300  = Precise but loses context (good for Q&A, chat logs)
#   500-800  = Balanced, recommended for general documents
#   1000-2000 = More context but less precise (good for long-form content)
CHUNK_SIZE=700

# Overlap between adjacent chunks in characters
# Guidelines:
#   10-15% of chunk_size = Minimal, fast indexing
#   15-25% of chunk_size = Balanced (recommended)
#   25-30% of chunk_size = Maximum context preservation
# Example: For CHUNK_SIZE=700, use 105-210
CHUNK_OVERLAP=150


# ============================================================================
# 4. EMBEDDING CONFIGURATION
# ============================================================================
# Vector embeddings for semantic search

# Embedding model from HuggingFace
# Options:
#   BAAI/bge-small-en        = Fast, 384 dims (recommended for laptops)
#   BAAI/bge-large-en        = Better quality, 1024 dims (more VRAM)
#   BAAI/bge-m3              = Multilingual (FR/EN/etc), 1024 dims
#   sentence-transformers/all-MiniLM-L6-v2 = Fast, 384 dims
EMBED_MODEL=BAAI/bge-small-en

# Embedding dimensions (must match model)
# bge-small-en: 384, bge-large-en: 1024, bge-m3: 1024, all-MiniLM: 384
EMBED_DIM=384

# Batch size for embedding (affects speed and memory)
# Guidelines:
#   M1 Mac (16GB):      32-64
#   M1 Max/Ultra:       64-128
#   NVIDIA GPU (24GB):  128-256
# Larger = faster but more memory
EMBED_BATCH=32

# Embedding backend
# Options:
#   huggingface = CPU/GPU with PyTorch (default, works everywhere)
#   mlx         = Apple Silicon optimized (M1/M2/M3 only, 5-20x faster!)
#   torch       = Explicit PyTorch (auto-detects CUDA/MPS)
# Performance: MLX > PyTorch GPU > PyTorch CPU > HuggingFace
EMBED_BACKEND=huggingface


# ============================================================================
# 5. LLM CONFIGURATION - BACKEND SELECTION
# ============================================================================
# Choose between llama.cpp (GGUF models) or vLLM (HuggingFace models)

# Use vLLM for GPU acceleration (0 or 1)
# 0 = llama.cpp (GGUF models, CPU/GPU, default)
# 1 = vLLM (HuggingFace models, GPU-only, 10-20x faster!)
# vLLM requires NVIDIA GPU (CUDA) or AMD GPU (ROCm)
USE_VLLM=0


# ============================================================================
# 5A. LLM CONFIGURATION - LLAMA.CPP (GGUF Models)
# ============================================================================
# Used when USE_VLLM=0 (default)

# Model URL for auto-download (GGUF format)
# Default: Mistral 7B Instruct Q4_K_M (4-bit quantized, ~4GB)
# Other options:
#   Q2_K = 2-bit, ~2.5GB (fastest, lowest quality)
#   Q4_K_M = 4-bit medium, ~4GB (balanced, recommended)
#   Q5_K_M = 5-bit medium, ~5GB (better quality)
#   Q8_0 = 8-bit, ~7GB (best quality)
MODEL_URL=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Local model path (if already downloaded)
# If set, MODEL_URL is ignored
# MODEL_PATH=/path/to/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU layers to offload (0 = CPU only)
# Guidelines:
#   M1 Mac (16GB):     16-24 layers
#   M1 Max (32GB):     24-32 layers
#   NVIDIA RTX 4090:   99 (all layers)
#   CPU only:          0
# Mistral 7B has 32 layers total
N_GPU_LAYERS=16

# Batch size for prompt processing (affects memory and speed)
# Guidelines:
#   M1 Mac (16GB):     128-256
#   M1 Max (32GB):     256-512
#   NVIDIA GPU (24GB): 512-1024
N_BATCH=128


# ============================================================================
# 5B. LLM CONFIGURATION - vLLM (HuggingFace Models)
# ============================================================================
# Used when USE_VLLM=1

# vLLM model name (HuggingFace format)
# AWQ quantized models recommended (4-bit, fast inference):
#   TheBloke/Mistral-7B-Instruct-v0.2-AWQ (~4GB VRAM)
#   TheBloke/Llama-2-7B-Chat-AWQ           (~4GB VRAM)
# Full precision models (FP16, more VRAM):
#   mistralai/Mistral-7B-Instruct-v0.2     (~14GB VRAM)
VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ

# vLLM server URL (for client mode)
# If using vLLM server instead of embedded mode
# VLLM_API_BASE=http://localhost:8000/v1

# vLLM server port (default: 8000)
VLLM_PORT=8000

# GPU memory utilization for vLLM (0.0-1.0)
# How much GPU memory vLLM should use
# 0.8 = 80% (safe default), 0.9 = 90% (aggressive)
# VLLM_GPU_MEMORY=0.8

# Tensor parallelism (number of GPUs to use)
# 1 = single GPU, 2+ = multi-GPU
# VLLM_TENSOR_PARALLEL=1

# Maximum context length for vLLM
# Model default used if not set
# VLLM_MAX_MODEL_LEN=8192


# ============================================================================
# 6. LLM CONFIGURATION - GENERATION SETTINGS
# ============================================================================
# Applies to both llama.cpp and vLLM

# Context window size (tokens)
# How much context (prompt + retrieved chunks) the LLM can see
# Guidelines:
#   3072  = Minimum for RAG (fits ~4 chunks)
#   8192  = Recommended (fits more context)
#   16384 = Large (requires more VRAM)
# Note: Must be <= model's max context (Mistral 7B: 8192)
CTX=3072

# Maximum tokens to generate
# How long the LLM's answer can be
# Guidelines:
#   128  = Short answers
#   256  = Medium answers (default)
#   512  = Long answers
#   1024 = Very detailed answers
MAX_NEW_TOKENS=256

# Temperature for sampling (0.0-2.0)
# Controls randomness in generation
# Guidelines:
#   0.0-0.1  = Deterministic, factual (recommended for RAG)
#   0.3-0.5  = Balanced
#   0.7-1.0  = Creative
#   1.0-2.0  = Very creative (not recommended for RAG)
TEMP=0.1

# Number of CPU threads (llama.cpp only)
# Auto-detected if not set
# N_THREADS=8


# ============================================================================
# 7. RETRIEVAL CONFIGURATION
# ============================================================================
# Controls how chunks are retrieved for each query

# Number of chunks to retrieve (1-20)
# Guidelines:
#   2-3 = Fast, focused answers
#   4-5 = Balanced (recommended)
#   6-10 = More context, slower generation
# Note: More chunks = larger prompt = slower generation
TOP_K=4

# Hybrid search alpha (0.0-1.0)
# Blends vector similarity with keyword search (BM25)
# 0.0 = Pure keyword search (BM25)
# 0.5 = Balanced hybrid
# 1.0 = Pure vector search (default)
# Hybrid search can improve recall for specific terms
HYBRID_ALPHA=1.0

# Enable metadata filtering (0 or 1)
# Allows filtering by document metadata (participant, date, etc.)
# 1 = enabled (default), 0 = disabled
ENABLE_FILTERS=1

# MMR diversity threshold (0.0-1.0)
# Maximum Marginal Relevance for diverse results
# 0.0 = Disabled (no diversity)
# 0.5 = Balanced (some diversity)
# 1.0 = Maximum diversity (may reduce relevance)
# Use to avoid retrieving near-duplicate chunks
MMR_THRESHOLD=0.0


# ============================================================================
# 8. PERFORMANCE TUNING (Advanced)
# ============================================================================
# Fine-tune performance and memory usage

# Database insert batch size
# How many nodes to insert at once
# Guidelines:
#   100  = Safe for constrained systems
#   250  = Balanced (default)
#   500+ = Faster but more memory
DB_INSERT_BATCH=250

# Extract chat metadata (0 or 1)
# Parse Facebook Messenger JSON for metadata (participant, date, etc.)
# 1 = extract metadata (for FB data), 0 = skip (default)
EXTRACT_CHAT_METADATA=0


# ============================================================================
# 9. LOGGING CONFIGURATION
# ============================================================================
# Control verbosity and debugging output

# Log level (DEBUG, INFO, WARNING, ERROR)
# DEBUG = Very verbose (shows all operations)
# INFO  = Normal (shows key operations)
# WARNING = Only warnings and errors
# ERROR = Only errors
LOG_LEVEL=INFO

# Log full chunk content (0 or 1)
# 1 = show complete retrieved chunks (very verbose)
# 0 = show truncated chunks (default)
LOG_FULL_CHUNKS=0

# Colorize chunk output (0 or 1)
# 1 = color-coded chunks in terminal (default)
# 0 = plain text (better for logs)
COLORIZE_CHUNKS=1

# Save query logs to JSON (0 or 1)
# 1 = save all queries to query_logs/ directory
# 0 = don't save (default)
LOG_QUERIES=0

# Disable Python output buffering (optional)
# Ensures logs appear immediately (useful for debugging)
# PYTHONUNBUFFERED=1


# ============================================================================
# 10. OPTIONAL FEATURES
# ============================================================================

# Default question for CLI (if not provided as argument)
# Used when running without --query flag
QUESTION=Summarize the key safety-related training ideas described in this paper.


# ============================================================================
# 11. EXTERNAL DEPENDENCIES (Optional)
# ============================================================================
# Configure cache locations for downloaded models

# HuggingFace cache directory
# Where HuggingFace stores downloaded models
# Default: ~/.cache/huggingface
# HF_HOME=/workspace/huggingface_cache
# TRANSFORMERS_CACHE=/workspace/huggingface_cache

# Transformers verbosity (error, warning, info, debug)
# Controls HuggingFace transformers logging
# TRANSFORMERS_VERBOSITY=error


# ============================================================================
# 12. DEVELOPMENT & TESTING
# ============================================================================
# Settings for development and debugging

# Enable resource monitoring (0 or 1)
# Track CPU/memory usage during operations
# ENABLE_RESOURCE_MONITORING=0

# Monitoring interval in seconds
# How often to sample resource usage
# MONITOR_INTERVAL=2


# ============================================================================
# QUICK START EXAMPLES
# ============================================================================
#
# MINIMAL SETUP (local, CPU-only):
#   PGUSER=myuser
#   PGPASSWORD=mypass
#   PDF_PATH=data/document.pdf
#
# OPTIMIZED FOR M1 MAC (16GB):
#   EMBED_BACKEND=mlx
#   N_GPU_LAYERS=24
#   N_BATCH=256
#   CTX=8192
#
# OPTIMIZED FOR NVIDIA GPU (RTX 4090):
#   USE_VLLM=1
#   VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ
#   EMBED_BACKEND=torch
#   EMBED_BATCH=256
#   N_GPU_LAYERS=99
#
# DEBUGGING:
#   LOG_LEVEL=DEBUG
#   LOG_FULL_CHUNKS=1
#   LOG_QUERIES=1
#
# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# Load environment:
#   source .env
#   # OR
#   export $(cat .env | xargs)
#
# Index documents:
#   python rag_low_level_m1_16gb_verbose.py
#
# Query existing index:
#   python rag_low_level_m1_16gb_verbose.py --query-only --query "your question"
#
# Interactive CLI:
#   python rag_interactive.py
#
# Web UI:
#   streamlit run rag_web.py
#
# Override specific variables:
#   CHUNK_SIZE=500 TOP_K=3 python rag_low_level_m1_16gb_verbose.py
#
# ============================================================================
