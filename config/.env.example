# ============================================================================
# Local RAG Pipeline - Comprehensive Environment Variables Template
# ============================================================================
#
# SETUP:
#   1. Copy this file to .env:
#      cp config/.env.example .env
#   2. Fill in your values (especially PGUSER and PGPASSWORD)
#   3. Load it: source .env  OR  export $(cat .env | xargs)
#
# SECURITY WARNING:
#   Never commit .env to git! It contains sensitive credentials.
#   The .env file is already in .gitignore.
#
# ============================================================================


# ============================================================================
# 1. DATABASE CONFIGURATION
# ============================================================================
# PostgreSQL connection settings (REQUIRED)

# Database host (localhost for local, IP/hostname for remote)
PGHOST=localhost

# PostgreSQL port (default: 5432)
PGPORT=5432

# Database username (REQUIRED - set your PostgreSQL user)
PGUSER=your_database_user

# Database password (REQUIRED - set your PostgreSQL password)
PGPASSWORD=your_secure_password_here

# Database name (will be auto-created if it doesn't exist)
DB_NAME=vector_db


# ============================================================================
# 2. DOCUMENT PROCESSING
# ============================================================================
# Source documents and index configuration

# Path to document(s) to index (file or folder)
# Supported: .pdf, .docx, .pptx, .html, .json, .txt, .md, .py, .cpp, etc.
PDF_PATH=data/llama2.pdf

# PostgreSQL table name for this index
# Leave unset for auto-generation based on config (recommended)
# Format: {filename}_cs{chunk_size}_ov{chunk_overlap}_{model_short}
# PGTABLE=my_custom_index

# Drop table before indexing (DESTRUCTIVE!)
# 0 = keep existing data, 1 = reset table
# Use 1 when changing chunk size/overlap to avoid mixed configs
RESET_TABLE=0

# Reset entire database (VERY DESTRUCTIVE!)
# 0 = normal operation, 1 = drop and recreate database
# WARNING: This deletes ALL tables in the database!
RESET_DB=0


# ============================================================================
# 3. CHUNKING CONFIGURATION (RAG Quality Tuning)
# ============================================================================
# Controls how documents are split into chunks for retrieval

# Chunk size in characters (100-2000)
# Guidelines:
#   100-300  = Precise but loses context (good for Q&A, chat logs)
#   500-800  = Balanced, recommended for general documents
#   1000-2000 = More context but less precise (good for long-form content)
CHUNK_SIZE=700

# Overlap between adjacent chunks in characters
# Guidelines:
#   10-15% of chunk_size = Minimal, fast indexing
#   15-25% of chunk_size = Balanced (recommended)
#   25-30% of chunk_size = Maximum context preservation
# Example: For CHUNK_SIZE=700, use 105-210
CHUNK_OVERLAP=150


# ============================================================================
# 4. EMBEDDING CONFIGURATION
# ============================================================================
# Vector embeddings for semantic search

# Embedding model from HuggingFace
# Options:
#   BAAI/bge-small-en        = Fast, 384 dims (recommended for laptops)
#   BAAI/bge-large-en        = Better quality, 1024 dims (more VRAM)
#   BAAI/bge-m3              = Multilingual (FR/EN/etc), 1024 dims
#   sentence-transformers/all-MiniLM-L6-v2 = Fast, 384 dims
EMBED_MODEL=BAAI/bge-small-en

# Embedding dimensions (must match model)
# bge-small-en: 384, bge-large-en: 1024, bge-m3: 1024, all-MiniLM: 384
EMBED_DIM=384

# Batch size for embedding (affects speed and memory)
# Guidelines:
#   M1 Mac (16GB):      64-128  (optimized for better throughput)
#   M1 Max/Ultra:       128-256
#   NVIDIA GPU (24GB):  256-512
# Larger = faster but more memory
# PERFORMANCE: Increased from 32 to 128 for 1.5x faster indexing
EMBED_BATCH=128

# Embedding backend
# Options:
#   huggingface = CPU/GPU with PyTorch (default, works everywhere)
#   mlx         = Apple Silicon optimized (M1/M2/M3 only, 5-20x faster!)
#   torch       = Explicit PyTorch (auto-detects CUDA/MPS)
# Performance: MLX > PyTorch GPU > PyTorch CPU > HuggingFace
EMBED_BACKEND=huggingface


# ============================================================================
# 5. LLM CONFIGURATION - BACKEND SELECTION
# ============================================================================
# Choose between llama.cpp (GGUF models) or vLLM (HuggingFace models)

# Use vLLM for GPU acceleration (0 or 1)
# 0 = llama.cpp (GGUF models, CPU/GPU, default)
# 1 = vLLM (HuggingFace models, GPU-only, 3-4x faster queries!)
# vLLM requires NVIDIA GPU (CUDA) or AMD GPU (ROCm)
# PERFORMANCE: Enable for 3-4x faster query responses (2-3s vs 8-15s)
# SETUP: Run ./scripts/start_vllm_server.sh in separate terminal first
USE_VLLM=0


# ============================================================================
# 5A. LLM CONFIGURATION - LLAMA.CPP (GGUF Models)
# ============================================================================
# Used when USE_VLLM=0 (default)

# Model URL for auto-download (GGUF format)
# Default: Mistral 7B Instruct Q4_K_M (4-bit quantized, ~4GB)
# Other options:
#   Q2_K = 2-bit, ~2.5GB (fastest, lowest quality)
#   Q4_K_M = 4-bit medium, ~4GB (balanced, recommended)
#   Q5_K_M = 5-bit medium, ~5GB (better quality)
#   Q8_0 = 8-bit, ~7GB (best quality)
MODEL_URL=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Local model path (if already downloaded)
# If set, MODEL_URL is ignored
# MODEL_PATH=/path/to/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU layers to offload (0 = CPU only)
# Guidelines:
#   M1 Mac (16GB):     24-32 layers  (optimized for better performance)
#   M1 Max (32GB):     32 layers (all)
#   NVIDIA RTX 4090:   99 (all layers)
#   CPU only:          0
# Mistral 7B has 32 layers total
# PERFORMANCE: Increased from 16 to 24 for better GPU utilization
N_GPU_LAYERS=24

# Batch size for prompt processing (affects memory and speed)
# Guidelines:
#   M1 Mac (16GB):     128-256
#   M1 Max (32GB):     256-512
#   NVIDIA GPU (24GB): 512-1024
N_BATCH=128


# ============================================================================
# 5B. LLM CONFIGURATION - vLLM (HuggingFace Models)
# ============================================================================
# Used when USE_VLLM=1

# vLLM model name (HuggingFace format)
# AWQ quantized models recommended (4-bit, fast inference):
#   TheBloke/Mistral-7B-Instruct-v0.2-AWQ (~4GB VRAM)
#   TheBloke/Llama-2-7B-Chat-AWQ           (~4GB VRAM)
# Full precision models (FP16, more VRAM):
#   mistralai/Mistral-7B-Instruct-v0.2     (~14GB VRAM)
VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ

# vLLM server URL (for client mode)
# If using vLLM server instead of embedded mode
# VLLM_API_BASE=http://localhost:8000/v1

# vLLM server port (default: 8000)
VLLM_PORT=8000

# GPU memory utilization for vLLM (0.0-1.0)
# How much GPU memory vLLM should use
# 0.8 = 80% (safe default), 0.9 = 90% (aggressive)
# VLLM_GPU_MEMORY=0.8

# Tensor parallelism (number of GPUs to use)
# 1 = single GPU, 2+ = multi-GPU
# VLLM_TENSOR_PARALLEL=1

# Maximum context length for vLLM
# Model default used if not set
# VLLM_MAX_MODEL_LEN=8192


# ============================================================================
# 6. LLM CONFIGURATION - GENERATION SETTINGS
# ============================================================================
# Applies to both llama.cpp and vLLM

# Context window size (tokens)
# How much context (prompt + retrieved chunks) the LLM can see
# Guidelines:
#   3072  = Minimum for RAG (fits ~4 chunks)
#   8192  = Recommended (fits more context)
#   16384 = Large (requires more VRAM)
# Note: Must be <= model's max context (Mistral 7B: 8192)
CTX=3072

# Maximum tokens to generate
# How long the LLM's answer can be
# Guidelines:
#   128  = Short answers
#   256  = Medium answers (default)
#   512  = Long answers
#   1024 = Very detailed answers
MAX_NEW_TOKENS=256

# Temperature for sampling (0.0-2.0)
# Controls randomness in generation
# Guidelines:
#   0.0-0.1  = Deterministic, factual (recommended for RAG)
#   0.3-0.5  = Balanced
#   0.7-1.0  = Creative
#   1.0-2.0  = Very creative (not recommended for RAG)
TEMP=0.1

# Number of CPU threads (llama.cpp only)
# Auto-detected if not set
# N_THREADS=8


# ============================================================================
# 7. RETRIEVAL CONFIGURATION
# ============================================================================
# Controls how chunks are retrieved for each query

# Number of chunks to retrieve (1-20)
# Guidelines:
#   2-3 = Fast, focused answers
#   4-5 = Balanced (recommended)
#   6-10 = More context, slower generation
# Note: More chunks = larger prompt = slower generation
TOP_K=4

# Hybrid search alpha (0.0-1.0)
# Blends vector similarity with keyword search (BM25)
# 0.0 = Pure keyword search (BM25)
# 0.5 = Balanced hybrid
# 1.0 = Pure vector search (default)
# Hybrid search can improve recall for specific terms
HYBRID_ALPHA=1.0

# Enable metadata filtering (0 or 1)
# Allows filtering by document metadata (participant, date, etc.)
# 1 = enabled (default), 0 = disabled
ENABLE_FILTERS=1

# MMR diversity threshold (0.0-1.0)
# Maximum Marginal Relevance for diverse results
# 0.0 = Disabled (no diversity)
# 0.5 = Balanced (some diversity)
# 1.0 = Maximum diversity (may reduce relevance)
# Use to avoid retrieving near-duplicate chunks
MMR_THRESHOLD=0.0

# HyDE (Hypothetical Document Embeddings) retrieval (0 or 1)
# Improves retrieval by generating hypothetical answers before embedding
# Process: Query -> LLM generates hypothetical answer -> Embed hypothesis -> Retrieve
# Benefits: 10-20% better retrieval for technical/complex queries
# Trade-off: +100-400ms latency for hypothesis generation
# 1 = enabled, 0 = disabled (default)
ENABLE_HYDE=0

# Number of hypothetical answers to generate (1-3)
# More hypotheses = better coverage but slower
# Guidelines:
#   1 = Fast, single perspective (+100-200ms)
#   2 = Balanced, dual perspectives (+200-300ms)
#   3 = Maximum coverage (+300-400ms)
# Requires ENABLE_HYDE=1
HYDE_NUM_HYPOTHESES=1

# Target length of each hypothesis in tokens (50-200)
# Shorter = faster generation, longer = more context
# Recommended: 100 tokens (~75 words)
HYDE_HYPOTHESIS_LENGTH=100

# Fusion method for combining multiple hypotheses
# How to merge results from multiple hypothetical answers
# Options:
#   rrf = Reciprocal Rank Fusion (recommended, balanced)
#   avg = Average scores (simple)
#   max = Maximum score (aggressive)
HYDE_FUSION_METHOD=rrf


# ============================================================================
# 8. PERFORMANCE TUNING (Advanced)
# ============================================================================
# Fine-tune performance and memory usage

# Database insert batch size
# How many nodes to insert at once
# Guidelines:
#   100  = Safe for constrained systems
#   250  = Balanced
#   500  = Faster (optimized, recommended for 16GB+ RAM)
#   1000+ = Fastest but more memory
# PERFORMANCE: Increased from 250 to 500 for faster bulk inserts
DB_INSERT_BATCH=500

# Extract chat metadata (0 or 1)
# Parse Facebook Messenger JSON for metadata (participant, date, etc.)
# 1 = extract metadata (for FB data), 0 = skip (default)
EXTRACT_CHAT_METADATA=0

# Enhanced metadata extraction (0 or 1)
# Extract rich metadata for improved RAG retrieval:
#   - Structure: Headings, doc type, section titles
#   - Semantic: Keywords, topics (TF-IDF), named entities
#   - Technical: Code blocks, tables, equations, function/class names
#   - Quality: Word count, reading level, sentence statistics
# 1 = enable all enhanced extraction (default)
# 0 = disable (use basic extractor only, faster)
EXTRACT_ENHANCED_METADATA=1

# Extract topics using TF-IDF (0 or 1)
# Compute semantic topics from text using TF-IDF
# Most expensive operation (~10ms per chunk)
# 1 = enabled (default), 0 = disabled (faster)
EXTRACT_TOPICS=1

# Extract named entities (0 or 1)
# Detect technical entities (Python, PyTorch, AWS, etc.) and proper nouns
# 1 = enabled (default), 0 = disabled (slightly faster)
EXTRACT_ENTITIES=1

# Extract code blocks (0 or 1)
# Detect code blocks, functions, classes, imports in documents/code
# 1 = enabled (default), 0 = disabled
EXTRACT_CODE_BLOCKS=1

# Extract tables (0 or 1)
# Detect tables in Markdown, HTML, or space-separated formats
# 1 = enabled (default), 0 = disabled
EXTRACT_TABLES=1


# ============================================================================
# 9. LOGGING CONFIGURATION
# ============================================================================
# Control verbosity and debugging output

# Log level (DEBUG, INFO, WARNING, ERROR)
# DEBUG = Very verbose (shows all operations)
# INFO  = Normal (shows key operations)
# WARNING = Only warnings and errors
# ERROR = Only errors
LOG_LEVEL=INFO

# Log full chunk content (0 or 1)
# 1 = show complete retrieved chunks (very verbose)
# 0 = show truncated chunks (default)
LOG_FULL_CHUNKS=0

# Colorize chunk output (0 or 1)
# 1 = color-coded chunks in terminal (default)
# 0 = plain text (better for logs)
COLORIZE_CHUNKS=1

# Save query logs to JSON (0 or 1)
# 1 = save all queries to query_logs/ directory
# 0 = don't save (default)
LOG_QUERIES=0

# Disable Python output buffering (optional)
# Ensures logs appear immediately (useful for debugging)
# PYTHONUNBUFFERED=1


# ============================================================================
# 10. OPTIONAL FEATURES
# ============================================================================

# Default question for CLI (if not provided as argument)
# Used when running without --query flag
QUESTION=Summarize the key safety-related training ideas described in this paper.


# ============================================================================
# 11. EXTERNAL DEPENDENCIES (Optional)
# ============================================================================
# Configure cache locations for downloaded models

# HuggingFace cache directory
# Where HuggingFace stores downloaded models
# Default: ~/.cache/huggingface
# HF_HOME=/workspace/huggingface_cache
# TRANSFORMERS_CACHE=/workspace/huggingface_cache

# Transformers verbosity (error, warning, info, debug)
# Controls HuggingFace transformers logging
# TRANSFORMERS_VERBOSITY=error


# ============================================================================
# 12. ANSWER VALIDATION (Quality Assurance)
# ============================================================================
# Validate RAG answers for confidence, hallucinations, and citations

# Enable answer validation (0 or 1)
# 1 = validate all answers (default), 0 = skip validation
ENABLE_ANSWER_VALIDATION=1

# Confidence threshold (0.0-1.0)
# Warn if answer confidence falls below this value
# 0.7 = require 70% confidence (default)
# Higher = stricter, Lower = more lenient
CONFIDENCE_THRESHOLD=0.7

# Hallucination threshold (0.0-1.0)
# Flag claims with similarity to context below this value
# 0.5 = require 50% similarity (default)
# Higher = stricter (fewer false positives), Lower = more sensitive
HALLUCINATION_THRESHOLD=0.5

# Relevance threshold (0.0-1.0)
# Minimum query-answer similarity required
# 0.6 = require 60% relevance (default)
RELEVANCE_THRESHOLD=0.6

# Faithfulness threshold (0.0-1.0)
# Minimum answer-context grounding required
# 0.7 = require 70% faithfulness (default)
FAITHFULNESS_THRESHOLD=0.7


# ============================================================================
# 13. PERFORMANCE OPTIMIZATIONS (Async & Pooling)
# ============================================================================
# Advanced performance features for async operations and connection pooling

# Enable async operations (0 or 1)
# Use async/await for embeddings and database operations
# Provides 2-5x speedup for batch operations
# 1 = enabled (default), 0 = disabled (fallback to sync)
ENABLE_ASYNC=1

# Database connection pool size
# Number of persistent database connections to maintain
# Guidelines:
#   5-10  = Laptop/desktop (low concurrency)
#   10-20 = Server (moderate concurrency)
#   20-50 = High-traffic server (high concurrency)
# Reduces connection overhead by 80%
CONNECTION_POOL_SIZE=10

# Minimum pool size
# Minimum number of connections to keep alive
MIN_POOL_SIZE=5

# Maximum pool size
# Maximum number of connections allowed
MAX_POOL_SIZE=20

# Batch size for async operations
# Number of queries to process together in a batch
# Guidelines:
#   16-32  = Standard (balanced)
#   32-64  = High throughput
#   64-128 = Maximum throughput (more memory)
# Provides 2-3x speedup through batch processing
BATCH_SIZE=32

# Batch timeout (seconds)
# Maximum time to wait before processing incomplete batch
# Guidelines:
#   0.5-1.0 = Low latency (process quickly)
#   1.0-2.0 = Balanced
#   2.0-5.0 = High throughput (wait for full batches)
BATCH_TIMEOUT=1.0


# ============================================================================
# 13. DEVELOPMENT & TESTING
# ============================================================================
# Settings for development and debugging

# Enable resource monitoring (0 or 1)
# Track CPU/memory usage during operations
# ENABLE_RESOURCE_MONITORING=0

# Monitoring interval in seconds
# How often to sample resource usage
# MONITOR_INTERVAL=2


# ============================================================================
# 11. GRAFANA MONITORING
# ============================================================================
# Grafana dashboard integration and MCP server configuration

# Grafana admin credentials (set in docker-compose.yml)
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# Grafana service account token for MCP integration
# Create in Grafana UI: Administration > Service Accounts > Add service account
# Required scopes: dashboards:read, datasources:read, datasources:query, annotations:read
# Leave empty until you generate a token in Grafana
GRAFANA_SERVICE_ACCOUNT_TOKEN=


# ============================================================================
# QUICK START EXAMPLES
# ============================================================================
#
# MINIMAL SETUP (local, CPU-only):
#   PGUSER=myuser
#   PGPASSWORD=mypass
#   PDF_PATH=data/document.pdf
#
# OPTIMIZED FOR M1 MAC (16GB):
#   EMBED_BACKEND=mlx
#   N_GPU_LAYERS=24
#   N_BATCH=256
#   CTX=8192
#
# OPTIMIZED FOR NVIDIA GPU (RTX 4090):
#   USE_VLLM=1
#   VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ
#   EMBED_BACKEND=torch
#   EMBED_BATCH=256
#   N_GPU_LAYERS=99
#
# DEBUGGING:
#   LOG_LEVEL=DEBUG
#   LOG_FULL_CHUNKS=1
#   LOG_QUERIES=1
#
# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# Load environment:
#   source .env
#   # OR
#   export $(cat .env | xargs)
#
# Index documents:
#   python rag_low_level_m1_16gb_verbose.py
#
# Query existing index:
#   python rag_low_level_m1_16gb_verbose.py --query-only --query "your question"
#
# Interactive CLI:
#   python rag_interactive.py
#
# Web UI:
#   streamlit run rag_web.py
#
# Override specific variables:
#   CHUNK_SIZE=500 TOP_K=3 python rag_low_level_m1_16gb_verbose.py
#
# ============================================================================
