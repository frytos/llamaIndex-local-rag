# ==========================================
# Runpod vLLM Configuration - Optimized for RTX 4090
# ==========================================
# Source this file: source runpod_vllm_config.env

# ==========================================
# LLM Backend Selection
# ==========================================
export USE_VLLM=1                           # Use vLLM (GPU-optimized, 15x faster)
export VLLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-AWQ  # AWQ quantized (4GB)

# Alternative models:
# export VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2  # Original (14GB)
# export VLLM_MODEL=TheBloke/Llama-2-7B-Chat-AWQ        # Llama 2 (4GB)

# ==========================================
# GPU Configuration
# ==========================================
export EMBED_BACKEND=torch                  # Use PyTorch for embeddings (GPU)
export N_GPU_LAYERS=99                      # Fallback for llama.cpp (if USE_VLLM=0)
export CUDA_VISIBLE_DEVICES=0               # Use first GPU

# ==========================================
# Embedding Configuration (GPU-Optimized)
# ==========================================
export EMBED_MODEL=BAAI/bge-small-en        # Fast, good quality
export EMBED_DIM=384
export EMBED_BATCH=128                      # 4x larger than M1 (more VRAM!)

# For better quality (slower but better retrieval):
# export EMBED_MODEL=BAAI/bge-large-en      # Best quality EN
# export EMBED_DIM=1024
# export EMBED_BATCH=64

# For multilingual (FR/EN):
# export EMBED_MODEL=BAAI/bge-m3            # Best for bilingual
# export EMBED_DIM=1024
# export EMBED_BATCH=64

# ==========================================
# PostgreSQL Configuration
# ==========================================
export PGHOST=localhost
export PGPORT=5432
export PGUSER=fryt
export PGPASSWORD=frytos
export DB_NAME=vector_db

# ==========================================
# RAG Configuration
# ==========================================
export CHUNK_SIZE=700                       # Optimal chunk size
export CHUNK_OVERLAP=150                    # 21% overlap
export TOP_K=5                              # Retrieve 5 chunks (vs 4 default)
export CTX=8192                             # Larger context (RTX 4090 can handle it!)
export MAX_NEW_TOKENS=512                   # Longer answers (2x default)
export TEMP=0.1                             # Factual/deterministic

# ==========================================
# Advanced Retrieval (Optional)
# ==========================================
export HYBRID_ALPHA=0.5                     # 50% vector, 50% keyword (balanced)
export MMR_THRESHOLD=0.5                    # Diversity threshold
export ENABLE_FILTERS=1                     # Enable metadata filtering

# ==========================================
# Logging & Debug
# ==========================================
export LOG_LEVEL=INFO                       # INFO (default) or DEBUG (verbose)
export LOG_FULL_CHUNKS=1                    # Show full retrieved chunks
export COLORIZE_CHUNKS=1                    # Color-coded chunks
export LOG_QUERIES=1                        # Log all queries to file

# ==========================================
# Performance Monitoring
# ==========================================
export ENABLE_RESOURCE_MONITORING=1         # Track memory/CPU usage
export MONITOR_INTERVAL=2                   # Seconds between samples

# ==========================================
# Data Paths
# ==========================================
export PDF_PATH=/workspace/rag-pipeline/data/messenger_clean_small
export PGTABLE=messenger_gpu_vllm           # Different table for vLLM results

# ==========================================
# HuggingFace Cache (Optional)
# ==========================================
export HF_HOME=/workspace/huggingface_cache
export TRANSFORMERS_CACHE=/workspace/huggingface_cache

# ==========================================
# vLLM Advanced Settings (Optional)
# ==========================================
# export VLLM_GPU_MEMORY=0.9                # Use 90% GPU memory (default: 0.8)
# export VLLM_TENSOR_PARALLEL=1             # Number of GPUs (default: 1)
# export VLLM_MAX_MODEL_LEN=8192            # Max context (default: model's max)

# ==========================================
# Quick Commands
# ==========================================
# Index data:
#   python3 rag_low_level_m1_16gb_verbose.py
#
# Query:
#   python3 rag_low_level_m1_16gb_verbose.py --query-only --query "your question"
#
# Benchmark:
#   ./scripts/test_query_quality.sh
#
# Monitor GPU:
#   watch -n 0.5 nvidia-smi

# ==========================================
# Performance Expectations (RTX 4090)
# ==========================================
# Embedding: ~500-800 chunks/sec (vs ~67 on M1)
# LLM vLLM: ~120-150 tokens/sec (vs ~10 on M1)
# Query time: ~5-10s (vs ~65s on M1)
# Indexing 58k chunks: ~2-3 min (vs ~15 min on M1)

echo "âœ… vLLM config loaded!"
echo "   Backend: vLLM GPU (15x faster)"
echo "   Model: $VLLM_MODEL"
echo "   Embedding: $EMBED_MODEL on GPU"
echo "   Table: $PGTABLE"
